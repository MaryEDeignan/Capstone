{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCJp0WNW8_VG"
   },
   "source": [
    "# Homework 3\n",
    "You will use the PyTorch library for all the classes and methods in this assignment. You should use a double-precision data type, and the device is either \"CPU\" or \"cuda.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jXPyUIQf42Pj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.linalg import toeplitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing classes\n",
    "import SCAD_Class as SCAD\n",
    "import Elastic_Net_Class as EN\n",
    "import SQRT_Lasso_Class as SQRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buECI5HHSXIB"
   },
   "source": [
    "## Part 1\n",
    "1) Create your own PyTorch class that implements the method of SCAD regularization and variable selection (smoothly clipped absolute deviations) for linear models. Your development should be based on the following references:\n",
    "\n",
    "- https://andrewcharlesjones.github.io/journal/scad.html\n",
    "- https://www.jstor.org/stable/27640214?seq=1\n",
    "\n",
    " Test your method on a real data set and determine a variable selection based on features' importance, according to SCAD. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "tr_G0wNB5cma",
    "outputId": "c058db1a-1fbb-4a48-8312-92cf5851425f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crime</th>\n",
       "      <th>residential</th>\n",
       "      <th>industrial</th>\n",
       "      <th>nox</th>\n",
       "      <th>rooms</th>\n",
       "      <th>older</th>\n",
       "      <th>distance</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>lstat</th>\n",
       "      <th>cmedv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.199997</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>296</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.900002</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>242</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     crime  residential  industrial    nox  rooms      older  distance  tax  \\\n",
       "0  0.00632         18.0        2.31  0.538  6.575  65.199997    4.0900  296   \n",
       "1  0.02731          0.0        7.07  0.469  6.421  78.900002    4.9671  242   \n",
       "\n",
       "     ptratio  lstat  cmedv  \n",
       "0  15.300000   4.98   24.0  \n",
       "1  17.799999   9.14   21.6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing housing dataset from github\n",
    "housing = pd.read_csv('https://raw.githubusercontent.com/dvasiliu/AML/refs/heads/main/Data%20Sets/housing.csv')\n",
    "housing = housing.drop(columns = ['town', 'tract','longitude','latitude','river','highway']) # removing categorical/string features\n",
    "housing.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O0gLKXzyJaD7"
   },
   "outputs": [],
   "source": [
    "# setting x and y variables with y being CMEDV which is Median value of owner-occupied homes in $1000's\n",
    "X = housing.drop(columns=['cmedv'])\n",
    "y = housing['cmedv']\n",
    "\n",
    "#scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# turning inputs for x and y into tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float64)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wu8NDpLdJl8b"
   },
   "outputs": [],
   "source": [
    "# performing tts on data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05UVBff6JpLt",
    "outputId": "491ee11a-f8e9-4735-d6b6-41582ffadf4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100/2000, Loss: 373.73065308898913\n",
      "epoch: 200/2000, Loss: 257.72626911325625\n",
      "epoch: 300/2000, Loss: 180.99697091665817\n",
      "epoch: 400/2000, Loss: 129.57883221085012\n",
      "epoch: 500/2000, Loss: 95.02195591536142\n",
      "epoch: 600/2000, Loss: 71.7702101340218\n",
      "epoch: 700/2000, Loss: 56.109388799087384\n",
      "epoch: 800/2000, Loss: 45.548529402143586\n",
      "epoch: 900/2000, Loss: 38.41573110746524\n",
      "epoch: 1000/2000, Loss: 33.58843749985296\n",
      "epoch: 1100/2000, Loss: 30.31271051280762\n",
      "epoch: 1200/2000, Loss: 28.082024553862205\n",
      "epoch: 1300/2000, Loss: 26.555952750683883\n",
      "epoch: 1400/2000, Loss: 25.505604034961078\n",
      "epoch: 1500/2000, Loss: 24.777003111709643\n",
      "epoch: 1600/2000, Loss: 24.26650777711772\n",
      "epoch: 1700/2000, Loss: 23.90430325105072\n",
      "epoch: 1800/2000, Loss: 23.643317190722854\n",
      "epoch: 1900/2000, Loss: 23.451773370586448\n",
      "epoch: 2000/2000, Loss: 23.308188336266586\n",
      "Coefficients: [[-0.81646896  0.65674727 -0.66842402 -0.76813385  2.75073084 -0.23592049\n",
      "  -2.33653034 -0.31554764 -1.53764449 -3.7154121 ]]\n"
     ]
    }
   ],
   "source": [
    "#setting a random seed so results are reproducible\n",
    "torch.manual_seed(440)\n",
    "\n",
    "# initializing the SCADLinearRegression model from .py file\n",
    "model = SCAD.SCADLinearRegression(input_size=X_train.shape[1], lambda_val=.01, a_val=2.5)\n",
    "\n",
    "#fitting the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# getting the coefficients learned by the model after training\n",
    "coefficients = model.get_coefficients().detach().cpu().numpy()\n",
    "print(\"Coefficients:\", coefficients)\n",
    "\n",
    "# making predictions on the test set (X_test)\n",
    "predictions = model.predict(X_test)\n",
    "#print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z06h7M_tOlY1"
   },
   "source": [
    "### Findings\n",
    "\n",
    "The coefficients as found above rounded: \n",
    "- crime:&nbsp;&nbsp; -0.816\n",
    "- residential:&nbsp;&nbsp;  0.657\n",
    "- industrial:&nbsp;&nbsp;   -0.668\n",
    "- nox:&nbsp;&nbsp;   -0.768\n",
    "- rooms:&nbsp;&nbsp;   2.751\t\n",
    "- older:&nbsp;&nbsp;   -0.236\n",
    "- distance:&nbsp;&nbsp;   -2.337\t\n",
    "- tax:&nbsp;&nbsp;   -0.316\t\n",
    "- ptratio:&nbsp;&nbsp;   -1.538\n",
    "- lstat:&nbsp;&nbsp;   -3.715\n",
    "\n",
    "The positive coefficients indicate that an increase in the feature corresponds to an increase in predicted median home value (ex. as rooms increases, so does predicted median home value).\n",
    "\n",
    "The negative coefficients indicate than an increase in the feature corresponds to a decrease in predicted median home value (ex. as distance increases, predicted median home value decreases).\n",
    "\n",
    "Based on these coefficients, we can determine that rooms, ptratio, and lstat are all very significant variables in predicting median home value and should therefore stay in our model. None of the variables have zero-coefficients, meaning they are all contributing something to the model, so all of the features should likely stay; however, the features older and tax are contribting the least to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATrmYJnicto2"
   },
   "source": [
    "## Part 2\n",
    "\n",
    "Based on the simulation design explained in class, generate 200 data sets where the input features have a strong correlation structure (you may consider a 0.9) and apply ElasticNet, SqrtLasso and SCAD to check which method produces the best approximation of an ideal solution, such as a \"betastar\" you design with a sparsity pattern of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s60Ylw6Fhal8"
   },
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yiJKPvR5eqEl"
   },
   "outputs": [],
   "source": [
    "# Generating 200 data sets where inputs have correlation of .9, this code is based on Penalized Regression with PyTorch notebook example.\n",
    "def make_correlated_features(num_samples,p,rho):\n",
    "  vcor = []\n",
    "  for i in range(p):\n",
    "    vcor.append(rho**i)\n",
    "  r = toeplitz(vcor)\n",
    "  mu = np.repeat(0,p)\n",
    "  x = np.random.multivariate_normal(mu, r, size=num_samples)\n",
    "  return x\n",
    "\n",
    "rho =0.9 # correlation\n",
    "p = 15 # number of features\n",
    "n = 200 # number of samples\n",
    "vcor = []\n",
    "for i in range(p):\n",
    "  vcor.append(rho**i)\n",
    "\n",
    "\n",
    "x = make_correlated_features(n,p,rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8d_ROdbPgisy"
   },
   "outputs": [],
   "source": [
    "beta =np.array([-1,0,0,2,2,-3,-1,0,4,3]) # setting true coefficients for model\n",
    "beta = beta.reshape(-1,1) # reshaping to be column vector\n",
    "\n",
    "# filling the rest of betas with zeros so there is a coefficient for each feature (we have 15 features and only set 10 coefficients)\n",
    "betastar = np.concatenate([beta,np.repeat(0,p-len(beta)).reshape(-1,1)],axis=0) \n",
    "# finding y variable with added noise\n",
    "y = x@betastar + 1.5*np.random.normal(size=(n,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zs9DlQ2thmfd"
   },
   "outputs": [],
   "source": [
    "# creating tensors from x and y with double precision\n",
    "x_tensor = torch.tensor(x, dtype=torch.float64)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KiMCFp0cimPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 4.0788044983678695\n",
      "Epoch [200/1000], Loss: 3.3471490901845047\n",
      "Epoch [300/1000], Loss: 2.9872301954440426\n",
      "Epoch [400/1000], Loss: 2.782351525455871\n",
      "Epoch [500/1000], Loss: 2.6601109951214554\n",
      "Epoch [600/1000], Loss: 2.585032518349653\n",
      "Epoch [700/1000], Loss: 2.538231926826157\n",
      "Epoch [800/1000], Loss: 2.507820937282077\n",
      "Epoch [900/1000], Loss: 2.4876015396762625\n",
      "Epoch [1000/1000], Loss: 2.4738721998072113\n",
      "Epoch [100/1000], Loss: 3.730303954390288\n",
      "Epoch [200/1000], Loss: 3.30325462676298\n",
      "Epoch [300/1000], Loss: 3.1027822201472355\n",
      "Epoch [400/1000], Loss: 2.957817642838785\n",
      "Epoch [500/1000], Loss: 2.850349607047089\n",
      "Epoch [600/1000], Loss: 2.791300888173902\n",
      "Epoch [700/1000], Loss: 2.7585069369231796\n",
      "Epoch [800/1000], Loss: 2.741452735676023\n",
      "Epoch [900/1000], Loss: 2.735649723831341\n",
      "Epoch [1000/1000], Loss: 2.7341206345615765\n",
      "epoch: 100/1000, Loss: 4.88192149069862\n",
      "epoch: 200/1000, Loss: 3.0706792908934686\n",
      "epoch: 300/1000, Loss: 2.3540865001322286\n",
      "epoch: 400/1000, Loss: 2.025782123952854\n",
      "epoch: 500/1000, Loss: 1.8612618655505446\n",
      "epoch: 600/1000, Loss: 1.7727938463675168\n",
      "epoch: 700/1000, Loss: 1.7217137684300754\n",
      "epoch: 800/1000, Loss: 1.6905145323998914\n",
      "epoch: 900/1000, Loss: 1.6704819125860086\n",
      "epoch: 1000/1000, Loss: 1.6570805117739769\n"
     ]
    }
   ],
   "source": [
    "# initializes and fits the model to the training data,then retrieves and flattens the learned coefficients into a numpy array\n",
    "\n",
    "elastic_net = EN.ElasticNet(input_size=p, alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(x_tensor, y_tensor)\n",
    "learned_coefficients_el = elastic_net.get_coefficients().detach().numpy().flatten()\n",
    "\n",
    "sqrt_lasso = SQRT.SqrtLasso(input_size=p, alpha = .1)\n",
    "sqrt_lasso.fit(x_tensor, y_tensor)\n",
    "learned_coefficients_sl = sqrt_lasso.get_coefficients().detach().numpy().flatten() \n",
    "\n",
    "scad_model = SCAD.SCADLinearRegression(input_size=p, lambda_val=.01, a_val=2.0)\n",
    "scad_model.fit(x_tensor, y_tensor, num_epochs=1000, learning_rate=.01)\n",
    "learned_coefficients_scad = scad_model.get_coefficients().detach().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Coefficients</th>\n",
       "      <th>ElasticNet Coefficients</th>\n",
       "      <th>SqrtLasso Coefficients</th>\n",
       "      <th>SCAD Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.5448</td>\n",
       "      <td>-0.4056</td>\n",
       "      <td>-0.8483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.0857</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.2434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.1573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.4218</td>\n",
       "      <td>1.2852</td>\n",
       "      <td>1.8875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0564</td>\n",
       "      <td>1.0026</td>\n",
       "      <td>1.9646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-3</td>\n",
       "      <td>-1.4141</td>\n",
       "      <td>-1.5824</td>\n",
       "      <td>-2.6797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.9920</td>\n",
       "      <td>-0.6410</td>\n",
       "      <td>-1.5573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3068</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.3167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>2.6237</td>\n",
       "      <td>2.8753</td>\n",
       "      <td>3.3314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>2.8595</td>\n",
       "      <td>3.4573</td>\n",
       "      <td>3.5132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5432</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.3465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.2294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.2595</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>-0.6809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>0.1043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    True Coefficients  ElasticNet Coefficients  SqrtLasso Coefficients  \\\n",
       "0                  -1                  -0.5448                 -0.4056   \n",
       "1                   0                  -0.0857                  0.0002   \n",
       "2                   0                   0.1342                  0.0020   \n",
       "3                   2                   1.4218                  1.2852   \n",
       "4                   2                   1.0564                  1.0026   \n",
       "5                  -3                  -1.4141                 -1.5824   \n",
       "6                  -1                  -0.9920                 -0.6410   \n",
       "7                   0                   0.3068                  0.0019   \n",
       "8                   4                   2.6237                  2.8753   \n",
       "9                   3                   2.8595                  3.4573   \n",
       "10                  0                   0.5432                  0.0027   \n",
       "11                  0                   0.3937                  0.0018   \n",
       "12                  0                   0.0008                  0.0003   \n",
       "13                  0                  -0.2595                  0.0012   \n",
       "14                  0                  -0.0007                 -0.0019   \n",
       "\n",
       "    SCAD Coefficients  \n",
       "0             -0.8483  \n",
       "1             -0.2434  \n",
       "2              0.1573  \n",
       "3              1.8875  \n",
       "4              1.9646  \n",
       "5             -2.6797  \n",
       "6             -1.5573  \n",
       "7              0.3167  \n",
       "8              3.3314  \n",
       "9              3.5132  \n",
       "10             0.0824  \n",
       "11             0.3465  \n",
       "12             0.2294  \n",
       "13            -0.6809  \n",
       "14             0.1043  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a dataframe to easily compare true coefficients with learned coefficients from three models\n",
    "coef_data = {\n",
    "    \"True Coefficients\": betastar.flatten(),\n",
    "    \"ElasticNet Coefficients\": learned_coefficients_el,\n",
    "    \"SqrtLasso Coefficients\": learned_coefficients_sl,\n",
    "    \"SCAD Coefficients\": learned_coefficients_scad\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(coef_data)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating plot to easily see the absolute differences from the true coefficient for each model\n",
    "abs_diff = df.sub(df[\"True Coefficients\"], axis=0).abs()\n",
    "color_scheme = plt.get_cmap('Dark2')\n",
    "abs_diff.drop(columns=[\"True Coefficients\"]).plot(kind='bar', figsize=(6, 4), alpha=0.7, color = color_scheme.colors)\n",
    "plt.title(\"Absolute Differences from True Coefficients\")\n",
    "plt.ylabel(\"Absolute Difference\")\n",
    "plt.xlabel(\"Coefficient Index\")\n",
    "plt.legend(fontsize='small')\n",
    "\n",
    "#plt.savefig('absolute_differences.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "The graph above illustrates the absolute difference from the defined coefficient for each model. Based on this graph, SCAD is generally the closest to the true coefficients when they are non-zero coefficients. Interestingly, SCAD failed to have zero coefficients where they were defined (at indices 1, 2, 7, 10, 11, 12, 13, and 14). Square Root Lasso did a very good job of finding these zero-coefficients and Elastic-Net did a notably better job that SCAD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Hk_qG59c7H4"
   },
   "source": [
    "## Part 3\n",
    "Use the methods you implemented above to determine a variable selection for the Concrete data set with quadratic interaction terms (polynomial features of degree 2). To solve this, you should consider choosing the best weight for the penalty function. What is the ideal model size (number of variables with non-zero weights), and what is the cross-validated mean square error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>ash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplastic</th>\n",
       "      <th>coarseagg</th>\n",
       "      <th>fineagg</th>\n",
       "      <th>age</th>\n",
       "      <th>strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>162.0000</td>\n",
       "      <td>2.5000</td>\n",
       "      <td>1040.0000</td>\n",
       "      <td>676.0000</td>\n",
       "      <td>28</td>\n",
       "      <td>79.9900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>162.0000</td>\n",
       "      <td>2.5000</td>\n",
       "      <td>1055.0000</td>\n",
       "      <td>676.0000</td>\n",
       "      <td>28</td>\n",
       "      <td>61.8900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cement   slag    ash    water  superplastic  coarseagg  fineagg  age  \\\n",
       "0 540.0000 0.0000 0.0000 162.0000        2.5000  1040.0000 676.0000   28   \n",
       "1 540.0000 0.0000 0.0000 162.0000        2.5000  1055.0000 676.0000   28   \n",
       "\n",
       "   strength  \n",
       "0   79.9900  \n",
       "1   61.8900  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete = pd.read_csv('https://raw.githubusercontent.com/dvasiliu/AAML/refs/heads/main/Data%20Sets/concrete.csv')\n",
    "concrete.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining X and y variables\n",
    "X = concrete.drop(columns=['strength']).values\n",
    "y = concrete['strength'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating polynomial features of degree 2 with interaction tea\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "# transforming X by generating polynomial features, adding new columns for quadratic and interaction terms\n",
    "X_poly = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data, MinMax Scaler had better results overall compared to StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "# fitting the scaler to the polynomial features and transforming\n",
    "X_scaled = scaler.fit_transform(X_poly)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=440)\n",
    "\n",
    "# 10 fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(model_class, X, y, penalty_value):\n",
    "    mse_list = []  # list to store mse from each fold\n",
    "    non_zero_weights_list = [] # list to store the number of non-zero weights from each fold - how many important features there are\n",
    "    \n",
    "    # performing 10 fold cross-validation\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        # splitting the data into training and testing sets for the current fold\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # converting the training and testing data into tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float64)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float64).reshape(-1, 1)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float64)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float64).reshape(-1, 1)\n",
    "        \n",
    "        # initialize and fit model\n",
    "        if model_class == EN.ElasticNet:\n",
    "            model = model_class(input_size=X_train.shape[1], l1_ratio=0.5, alpha=penalty_value)\n",
    "        elif model_class == SQRT.SqrtLasso:\n",
    "            model = model_class(input_size=X_train.shape[1], alpha=penalty_value)\n",
    "        else:  # SCAD\n",
    "            model = model_class(input_size=X_train.shape[1], lambda_val=penalty_value, a_val=2.5)\n",
    "\n",
    "        # fitting the model to the training data\n",
    "        model.fit(X_train_tensor, y_train_tensor, num_epochs=1000)\n",
    "\n",
    "        # calculate mse between predicted and actual values\n",
    "        y_pred = model.predict(X_test_tensor).detach().numpy()\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_list.append(mse) # storing mse for the fold\n",
    "        \n",
    "        # counting non-zero weights\n",
    "        non_zero_weights = (model.linear.weight.detach().numpy() != 0).sum()\n",
    "        non_zero_weights_list.append(non_zero_weights) # storing count for the fold\n",
    "    \n",
    "    # calculating average mse and average number of non-zero weights across all folds\n",
    "    avg_mse = np.mean(mse_list)\n",
    "    avg_non_zero_weights = np.mean(non_zero_weights_list)\n",
    "    \n",
    "    return avg_mse, avg_non_zero_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 103.44432507156942\n",
      "Epoch [200/1000], Loss: 86.61746860830088\n",
      "Epoch [300/1000], Loss: 77.22398585587557\n",
      "Epoch [400/1000], Loss: 70.9767018249634\n",
      "Epoch [500/1000], Loss: 66.60076497359627\n",
      "Epoch [600/1000], Loss: 63.407783952967506\n",
      "Epoch [700/1000], Loss: 60.9958314180719\n",
      "Epoch [800/1000], Loss: 59.11771787676388\n",
      "Epoch [900/1000], Loss: 57.61484053171668\n",
      "Epoch [1000/1000], Loss: 56.38219942342768\n",
      "Epoch [100/1000], Loss: 105.05977155456091\n",
      "Epoch [200/1000], Loss: 88.0728146482132\n",
      "Epoch [300/1000], Loss: 78.44747362404972\n",
      "Epoch [400/1000], Loss: 71.98119739712897\n",
      "Epoch [500/1000], Loss: 67.41621367085337\n",
      "Epoch [600/1000], Loss: 64.07127900996143\n",
      "Epoch [700/1000], Loss: 61.54360297493154\n",
      "Epoch [800/1000], Loss: 59.58081074503306\n",
      "Epoch [900/1000], Loss: 58.01889471037111\n",
      "Epoch [1000/1000], Loss: 56.74765024229382\n",
      "Epoch [100/1000], Loss: 102.7031546109585\n",
      "Epoch [200/1000], Loss: 86.21091195500053\n",
      "Epoch [300/1000], Loss: 76.89354383485498\n",
      "Epoch [400/1000], Loss: 70.61703045912752\n",
      "Epoch [500/1000], Loss: 66.17246130830293\n",
      "Epoch [600/1000], Loss: 62.90299774611329\n",
      "Epoch [700/1000], Loss: 60.42021715904236\n",
      "Epoch [800/1000], Loss: 58.4812152565399\n",
      "Epoch [900/1000], Loss: 56.92864452499662\n",
      "Epoch [1000/1000], Loss: 55.65679268816001\n",
      "Epoch [100/1000], Loss: 105.47866927668838\n",
      "Epoch [200/1000], Loss: 88.53433164314579\n",
      "Epoch [300/1000], Loss: 78.84691976004103\n",
      "Epoch [400/1000], Loss: 72.29956532682675\n",
      "Epoch [500/1000], Loss: 67.667189521916\n",
      "Epoch [600/1000], Loss: 64.27131603914206\n",
      "Epoch [700/1000], Loss: 61.70505008978293\n",
      "Epoch [800/1000], Loss: 59.71185729733517\n",
      "Epoch [900/1000], Loss: 58.124304465357596\n",
      "Epoch [1000/1000], Loss: 56.829423482278976\n",
      "Epoch [100/1000], Loss: 101.99582954911209\n",
      "Epoch [200/1000], Loss: 85.61303989017445\n",
      "Epoch [300/1000], Loss: 76.34357188054932\n",
      "Epoch [400/1000], Loss: 70.08771998558882\n",
      "Epoch [500/1000], Loss: 65.6450230107918\n",
      "Epoch [600/1000], Loss: 62.36669202910046\n",
      "Epoch [700/1000], Loss: 59.87032769056685\n",
      "Epoch [800/1000], Loss: 57.917217200395925\n",
      "Epoch [900/1000], Loss: 56.35169713284245\n",
      "Epoch [1000/1000], Loss: 55.06876167225578\n",
      "Epoch [100/1000], Loss: 103.7733921228369\n",
      "Epoch [200/1000], Loss: 87.33117885008961\n",
      "Epoch [300/1000], Loss: 77.98205418314664\n",
      "Epoch [400/1000], Loss: 71.66002498552751\n",
      "Epoch [500/1000], Loss: 67.16481706512785\n",
      "Epoch [600/1000], Loss: 63.84709309517208\n",
      "Epoch [700/1000], Loss: 61.32252081006494\n",
      "Epoch [800/1000], Loss: 59.3497551646778\n",
      "Epoch [900/1000], Loss: 57.77109739579665\n",
      "Epoch [1000/1000], Loss: 56.47934775599287\n",
      "Epoch [100/1000], Loss: 105.8116780290274\n",
      "Epoch [200/1000], Loss: 88.54508953211196\n",
      "Epoch [300/1000], Loss: 78.74037340447944\n",
      "Epoch [400/1000], Loss: 72.13698739975308\n",
      "Epoch [500/1000], Loss: 67.46256353691264\n",
      "Epoch [600/1000], Loss: 64.02812461566185\n",
      "Epoch [700/1000], Loss: 61.425842367174695\n",
      "Epoch [800/1000], Loss: 59.400027040076665\n",
      "Epoch [900/1000], Loss: 57.78389545471858\n",
      "Epoch [1000/1000], Loss: 56.46470921253032\n",
      "Epoch [100/1000], Loss: 104.83238069324958\n",
      "Epoch [200/1000], Loss: 88.01487757882127\n",
      "Epoch [300/1000], Loss: 78.68089592669246\n",
      "Epoch [400/1000], Loss: 72.48738843191657\n",
      "Epoch [500/1000], Loss: 68.13696538709297\n",
      "Epoch [600/1000], Loss: 64.94641780289194\n",
      "Epoch [700/1000], Loss: 62.522487804158615\n",
      "Epoch [800/1000], Loss: 60.62440336412831\n",
      "Epoch [900/1000], Loss: 59.09854908624986\n",
      "Epoch [1000/1000], Loss: 57.84317062203506\n",
      "Epoch [100/1000], Loss: 104.26837729279143\n",
      "Epoch [200/1000], Loss: 87.11602241362846\n",
      "Epoch [300/1000], Loss: 77.52659288591738\n",
      "Epoch [400/1000], Loss: 71.1333133327097\n",
      "Epoch [500/1000], Loss: 66.63130987129408\n",
      "Epoch [600/1000], Loss: 63.3284489705386\n",
      "Epoch [700/1000], Loss: 60.8230862032412\n",
      "Epoch [800/1000], Loss: 58.86733041120076\n",
      "Epoch [900/1000], Loss: 57.30183334765359\n",
      "Epoch [1000/1000], Loss: 56.019365383086736\n",
      "Epoch [100/1000], Loss: 102.50684394744782\n",
      "Epoch [200/1000], Loss: 85.99811409313882\n",
      "Epoch [300/1000], Loss: 76.77474846606192\n",
      "Epoch [400/1000], Loss: 70.62554793345299\n",
      "Epoch [500/1000], Loss: 66.3017798085297\n",
      "Epoch [600/1000], Loss: 63.13226261793573\n",
      "Epoch [700/1000], Loss: 60.726718399455656\n",
      "Epoch [800/1000], Loss: 58.84525809266364\n",
      "Epoch [900/1000], Loss: 57.33470820885115\n",
      "Epoch [1000/1000], Loss: 56.0933881811885\n",
      "Epoch [100/1000], Loss: 105.34669361915738\n",
      "Epoch [200/1000], Loss: 88.80501522217666\n",
      "Epoch [300/1000], Loss: 79.63836850121024\n",
      "Epoch [400/1000], Loss: 73.58367651119535\n",
      "Epoch [500/1000], Loss: 69.3733159845072\n",
      "Epoch [600/1000], Loss: 66.3244985001919\n",
      "Epoch [700/1000], Loss: 64.03976798946137\n",
      "Epoch [800/1000], Loss: 62.275192571560865\n",
      "Epoch [900/1000], Loss: 60.88260649213217\n",
      "Epoch [1000/1000], Loss: 59.750385174045505\n",
      "Epoch [100/1000], Loss: 106.44753102315802\n",
      "Epoch [200/1000], Loss: 89.87580965271589\n",
      "Epoch [300/1000], Loss: 80.55226282018329\n",
      "Epoch [400/1000], Loss: 74.32755101608764\n",
      "Epoch [500/1000], Loss: 69.96823310627049\n",
      "Epoch [600/1000], Loss: 66.80229865902224\n",
      "Epoch [700/1000], Loss: 64.43187198438729\n",
      "Epoch [800/1000], Loss: 62.608166728233016\n",
      "Epoch [900/1000], Loss: 61.17573771309551\n",
      "Epoch [1000/1000], Loss: 60.02136536283171\n",
      "Epoch [100/1000], Loss: 104.86933352520484\n",
      "Epoch [200/1000], Loss: 88.35582340680443\n",
      "Epoch [300/1000], Loss: 79.17660277276833\n",
      "Epoch [400/1000], Loss: 73.07480795792546\n",
      "Epoch [500/1000], Loss: 68.80472077273797\n",
      "Epoch [600/1000], Loss: 65.69769749656066\n",
      "Epoch [700/1000], Loss: 63.36255166391442\n",
      "Epoch [800/1000], Loss: 61.5569127768583\n",
      "Epoch [900/1000], Loss: 60.12826775485338\n",
      "Epoch [1000/1000], Loss: 58.97188901263042\n",
      "Epoch [100/1000], Loss: 106.9271129443013\n",
      "Epoch [200/1000], Loss: 90.35272999185142\n",
      "Epoch [300/1000], Loss: 80.9877128183091\n",
      "Epoch [400/1000], Loss: 74.71437774283979\n",
      "Epoch [500/1000], Loss: 70.31028048201432\n",
      "Epoch [600/1000], Loss: 67.10509781279217\n",
      "Epoch [700/1000], Loss: 64.70007809494372\n",
      "Epoch [800/1000], Loss: 62.845792360724154\n",
      "Epoch [900/1000], Loss: 61.38660587362552\n",
      "Epoch [1000/1000], Loss: 60.205269896335565\n",
      "Epoch [100/1000], Loss: 103.92539235743472\n",
      "Epoch [200/1000], Loss: 87.87056400172835\n",
      "Epoch [300/1000], Loss: 78.82797103857081\n",
      "Epoch [400/1000], Loss: 72.75564920958602\n",
      "Epoch [500/1000], Loss: 68.47118165017591\n",
      "Epoch [600/1000], Loss: 65.33279865570329\n",
      "Epoch [700/1000], Loss: 62.96179451698931\n",
      "Epoch [800/1000], Loss: 61.12168737319603\n",
      "Epoch [900/1000], Loss: 59.6644152863221\n",
      "Epoch [1000/1000], Loss: 58.48237555468244\n",
      "Epoch [100/1000], Loss: 105.21889012364653\n",
      "Epoch [200/1000], Loss: 89.00915368648387\n",
      "Epoch [300/1000], Loss: 79.90793609849295\n",
      "Epoch [400/1000], Loss: 73.82476951066998\n",
      "Epoch [500/1000], Loss: 69.54965182975198\n",
      "Epoch [600/1000], Loss: 66.43016579542342\n",
      "Epoch [700/1000], Loss: 64.08230559767307\n",
      "Epoch [800/1000], Loss: 62.2664672766794\n",
      "Epoch [900/1000], Loss: 60.83300394737297\n",
      "Epoch [1000/1000], Loss: 59.672325285326586\n",
      "Epoch [100/1000], Loss: 107.83888847643513\n",
      "Epoch [200/1000], Loss: 90.72171864900004\n",
      "Epoch [300/1000], Loss: 81.10332198643768\n",
      "Epoch [400/1000], Loss: 74.68709138017542\n",
      "Epoch [500/1000], Loss: 70.18638358934804\n",
      "Epoch [600/1000], Loss: 66.90922272040858\n",
      "Epoch [700/1000], Loss: 64.44847491326453\n",
      "Epoch [800/1000], Loss: 62.551447386934754\n",
      "Epoch [900/1000], Loss: 61.057955716566624\n",
      "Epoch [1000/1000], Loss: 59.849985711780214\n",
      "Epoch [100/1000], Loss: 106.98029914386989\n",
      "Epoch [200/1000], Loss: 90.23136068145071\n",
      "Epoch [300/1000], Loss: 81.02575353092566\n",
      "Epoch [400/1000], Loss: 74.98144499457375\n",
      "Epoch [500/1000], Loss: 70.77989165360287\n",
      "Epoch [600/1000], Loss: 67.73023597847744\n",
      "Epoch [700/1000], Loss: 65.43696468433998\n",
      "Epoch [800/1000], Loss: 63.659293080300806\n",
      "Epoch [900/1000], Loss: 62.24793641205575\n",
      "Epoch [1000/1000], Loss: 61.10114323620449\n",
      "Epoch [100/1000], Loss: 105.92514380180853\n",
      "Epoch [200/1000], Loss: 89.11540028195586\n",
      "Epoch [300/1000], Loss: 79.79022058312009\n",
      "Epoch [400/1000], Loss: 73.62694622362324\n",
      "Epoch [500/1000], Loss: 69.32234027306039\n",
      "Epoch [600/1000], Loss: 66.18959107292179\n",
      "Epoch [700/1000], Loss: 63.83235105446944\n",
      "Epoch [800/1000], Loss: 62.007080838570744\n",
      "Epoch [900/1000], Loss: 60.5645013750933\n",
      "Epoch [1000/1000], Loss: 59.392689250415515\n",
      "Epoch [100/1000], Loss: 104.07738021077996\n",
      "Epoch [200/1000], Loss: 87.85045191587007\n",
      "Epoch [300/1000], Loss: 78.88092678822075\n",
      "Epoch [400/1000], Loss: 72.96014580434182\n",
      "Epoch [500/1000], Loss: 68.83650407794293\n",
      "Epoch [600/1000], Loss: 65.84168849679583\n",
      "Epoch [700/1000], Loss: 63.589508158388526\n",
      "Epoch [800/1000], Loss: 61.84397824043929\n",
      "Epoch [900/1000], Loss: 60.45947676538932\n",
      "Epoch [1000/1000], Loss: 59.3337576413029\n",
      "Epoch [100/1000], Loss: 121.4060110361346\n",
      "Epoch [200/1000], Loss: 107.25775838170394\n",
      "Epoch [300/1000], Loss: 100.20222739369171\n",
      "Epoch [400/1000], Loss: 95.98394721772574\n",
      "Epoch [500/1000], Loss: 93.3237848945042\n",
      "Epoch [600/1000], Loss: 91.57449590114429\n",
      "Epoch [700/1000], Loss: 90.38260088547204\n",
      "Epoch [800/1000], Loss: 89.54459041662412\n",
      "Epoch [900/1000], Loss: 88.93858523926274\n",
      "Epoch [1000/1000], Loss: 88.4892204185989\n",
      "Epoch [100/1000], Loss: 121.7544773788107\n",
      "Epoch [200/1000], Loss: 107.78921686452809\n",
      "Epoch [300/1000], Loss: 100.6771848090097\n",
      "Epoch [400/1000], Loss: 96.35846472435223\n",
      "Epoch [500/1000], Loss: 93.60358069933403\n",
      "Epoch [600/1000], Loss: 91.77939715915933\n",
      "Epoch [700/1000], Loss: 90.53310597022174\n",
      "Epoch [800/1000], Loss: 89.65757507064086\n",
      "Epoch [900/1000], Loss: 89.02665779786625\n",
      "Epoch [1000/1000], Loss: 88.56132016735124\n",
      "Epoch [100/1000], Loss: 120.7830722077096\n",
      "Epoch [200/1000], Loss: 106.75129188450722\n",
      "Epoch [300/1000], Loss: 99.70556136840247\n",
      "Epoch [400/1000], Loss: 95.45034144423313\n",
      "Epoch [500/1000], Loss: 92.74083608287212\n",
      "Epoch [600/1000], Loss: 90.9451240197355\n",
      "Epoch [700/1000], Loss: 89.7148297483194\n",
      "Epoch [800/1000], Loss: 88.84695218918048\n",
      "Epoch [900/1000], Loss: 88.21844978183347\n",
      "Epoch [1000/1000], Loss: 87.75242397090565\n",
      "Epoch [100/1000], Loss: 122.82601347707893\n",
      "Epoch [200/1000], Loss: 108.65476853514761\n",
      "Epoch [300/1000], Loss: 101.4156657956162\n",
      "Epoch [400/1000], Loss: 97.01149890896485\n",
      "Epoch [500/1000], Loss: 94.20484916908698\n",
      "Epoch [600/1000], Loss: 92.35054803825766\n",
      "Epoch [700/1000], Loss: 91.08678320044311\n",
      "Epoch [800/1000], Loss: 90.20077536975197\n",
      "Epoch [900/1000], Loss: 89.56311032619179\n",
      "Epoch [1000/1000], Loss: 89.09295853830713\n",
      "Epoch [100/1000], Loss: 120.22791679835207\n",
      "Epoch [200/1000], Loss: 106.24507751660346\n",
      "Epoch [300/1000], Loss: 99.17792338866126\n",
      "Epoch [400/1000], Loss: 94.88470640492814\n",
      "Epoch [500/1000], Loss: 92.13398031644158\n",
      "Epoch [600/1000], Loss: 90.30064433627365\n",
      "Epoch [700/1000], Loss: 89.03890095462725\n",
      "Epoch [800/1000], Loss: 88.14610480455147\n",
      "Epoch [900/1000], Loss: 87.49855135014299\n",
      "Epoch [1000/1000], Loss: 87.01832738369758\n",
      "Epoch [100/1000], Loss: 121.54185547597196\n",
      "Epoch [200/1000], Loss: 107.47517400568333\n",
      "Epoch [300/1000], Loss: 100.32125246795847\n",
      "Epoch [400/1000], Loss: 95.98035858647452\n",
      "Epoch [500/1000], Loss: 93.20823286803126\n",
      "Epoch [600/1000], Loss: 91.36953759826338\n",
      "Epoch [700/1000], Loss: 90.11148134364177\n",
      "Epoch [800/1000], Loss: 89.22690650840116\n",
      "Epoch [900/1000], Loss: 88.58935346565232\n",
      "Epoch [1000/1000], Loss: 88.11932494584272\n",
      "Epoch [100/1000], Loss: 124.1272911463308\n",
      "Epoch [200/1000], Loss: 109.36320549552434\n",
      "Epoch [300/1000], Loss: 101.86941563732735\n",
      "Epoch [400/1000], Loss: 97.32733831860295\n",
      "Epoch [500/1000], Loss: 94.42821051147058\n",
      "Epoch [600/1000], Loss: 92.50573858109149\n",
      "Epoch [700/1000], Loss: 91.1902841675907\n",
      "Epoch [800/1000], Loss: 90.26496577033953\n",
      "Epoch [900/1000], Loss: 89.59752115807689\n",
      "Epoch [1000/1000], Loss: 89.1049081549949\n",
      "Epoch [100/1000], Loss: 122.33930844907212\n",
      "Epoch [200/1000], Loss: 108.2707374622582\n",
      "Epoch [300/1000], Loss: 101.2825332814217\n",
      "Epoch [400/1000], Loss: 97.100673916485\n",
      "Epoch [500/1000], Loss: 94.44874156564632\n",
      "Epoch [600/1000], Loss: 92.69132490313557\n",
      "Epoch [700/1000], Loss: 91.48371340535674\n",
      "Epoch [800/1000], Loss: 90.62756923241423\n",
      "Epoch [900/1000], Loss: 90.00373740388895\n",
      "Epoch [1000/1000], Loss: 89.53812921585359\n",
      "Epoch [100/1000], Loss: 121.30635772443773\n",
      "Epoch [200/1000], Loss: 107.12545243956984\n",
      "Epoch [300/1000], Loss: 100.01433785266474\n",
      "Epoch [400/1000], Loss: 95.73707385696521\n",
      "Epoch [500/1000], Loss: 93.01652191675034\n",
      "Epoch [600/1000], Loss: 91.21299234584893\n",
      "Epoch [700/1000], Loss: 89.97657353928938\n",
      "Epoch [800/1000], Loss: 89.1041116809504\n",
      "Epoch [900/1000], Loss: 88.47252035674887\n",
      "Epoch [1000/1000], Loss: 88.00473681129364\n",
      "Epoch [100/1000], Loss: 120.85106888917652\n",
      "Epoch [200/1000], Loss: 106.77148947310204\n",
      "Epoch [300/1000], Loss: 99.73609659709756\n",
      "Epoch [400/1000], Loss: 95.51794201063015\n",
      "Epoch [500/1000], Loss: 92.84692312153499\n",
      "Epoch [600/1000], Loss: 91.0820026640907\n",
      "Epoch [700/1000], Loss: 89.87369707088496\n",
      "Epoch [800/1000], Loss: 89.02066496794731\n",
      "Epoch [900/1000], Loss: 88.40195710033996\n",
      "Epoch [1000/1000], Loss: 87.94240638271381\n",
      "Epoch [100/1000], Loss: 256.5672219180674\n",
      "Epoch [200/1000], Loss: 253.42876923644104\n",
      "Epoch [300/1000], Loss: 252.8676244402808\n",
      "Epoch [400/1000], Loss: 252.73825278509082\n",
      "Epoch [500/1000], Loss: 252.70622365315296\n",
      "Epoch [600/1000], Loss: 252.69771387597672\n",
      "Epoch [700/1000], Loss: 252.69567369229043\n",
      "Epoch [800/1000], Loss: 252.6950247164549\n",
      "Epoch [900/1000], Loss: 252.69479496420422\n",
      "Epoch [1000/1000], Loss: 252.6947106847502\n",
      "Epoch [100/1000], Loss: 253.72377853413343\n",
      "Epoch [200/1000], Loss: 250.58686617029844\n",
      "Epoch [300/1000], Loss: 250.01843594898764\n",
      "Epoch [400/1000], Loss: 249.88687978889465\n",
      "Epoch [500/1000], Loss: 249.85326830274846\n",
      "Epoch [600/1000], Loss: 249.84496315895638\n",
      "Epoch [700/1000], Loss: 249.84222835180998\n",
      "Epoch [800/1000], Loss: 249.84125869692184\n",
      "Epoch [900/1000], Loss: 249.8416220116839\n",
      "Epoch [1000/1000], Loss: 249.84130476150204\n",
      "Epoch [100/1000], Loss: 255.05238064105203\n",
      "Epoch [200/1000], Loss: 251.756763824454\n",
      "Epoch [300/1000], Loss: 251.15163236591349\n",
      "Epoch [400/1000], Loss: 251.0103446306387\n",
      "Epoch [500/1000], Loss: 250.97505626481873\n",
      "Epoch [600/1000], Loss: 250.96575055549937\n",
      "Epoch [700/1000], Loss: 250.962703155022\n",
      "Epoch [800/1000], Loss: 250.96210633260233\n",
      "Epoch [900/1000], Loss: 250.96206562986998\n",
      "Epoch [1000/1000], Loss: 250.9616126964582\n",
      "Epoch [100/1000], Loss: 255.55412453617038\n",
      "Epoch [200/1000], Loss: 252.45664883006668\n",
      "Epoch [300/1000], Loss: 251.88778638697553\n",
      "Epoch [400/1000], Loss: 251.75305429709437\n",
      "Epoch [500/1000], Loss: 251.71938654729303\n",
      "Epoch [600/1000], Loss: 251.71028863042386\n",
      "Epoch [700/1000], Loss: 251.70767543114636\n",
      "Epoch [800/1000], Loss: 251.70726854367675\n",
      "Epoch [900/1000], Loss: 251.7069489009997\n",
      "Epoch [1000/1000], Loss: 251.70678867152594\n",
      "Epoch [100/1000], Loss: 252.21455381086759\n",
      "Epoch [200/1000], Loss: 249.04798948101353\n",
      "Epoch [300/1000], Loss: 248.46557206699538\n",
      "Epoch [400/1000], Loss: 248.32951237947805\n",
      "Epoch [500/1000], Loss: 248.29454103499708\n",
      "Epoch [600/1000], Loss: 248.2857041532699\n",
      "Epoch [700/1000], Loss: 248.2828088134952\n",
      "Epoch [800/1000], Loss: 248.2825171730313\n",
      "Epoch [900/1000], Loss: 248.2819948364859\n",
      "Epoch [1000/1000], Loss: 248.28165146730612\n",
      "Epoch [100/1000], Loss: 251.33692263768887\n",
      "Epoch [200/1000], Loss: 248.3081988770998\n",
      "Epoch [300/1000], Loss: 247.7568752574433\n",
      "Epoch [400/1000], Loss: 247.6305991831411\n",
      "Epoch [500/1000], Loss: 247.5984078695863\n",
      "Epoch [600/1000], Loss: 247.58978356057906\n",
      "Epoch [700/1000], Loss: 247.58733858467883\n",
      "Epoch [800/1000], Loss: 247.58659329955043\n",
      "Epoch [900/1000], Loss: 247.5863356623151\n",
      "Epoch [1000/1000], Loss: 247.58622266790695\n",
      "Epoch [100/1000], Loss: 256.5193306973698\n",
      "Epoch [200/1000], Loss: 253.3947502367059\n",
      "Epoch [300/1000], Loss: 252.8256684632323\n",
      "Epoch [400/1000], Loss: 252.69344645974746\n",
      "Epoch [500/1000], Loss: 252.65999827460865\n",
      "Epoch [600/1000], Loss: 252.6512540395034\n",
      "Epoch [700/1000], Loss: 252.64855031137023\n",
      "Epoch [800/1000], Loss: 252.6476297274269\n",
      "Epoch [900/1000], Loss: 252.6479450401493\n",
      "Epoch [1000/1000], Loss: 252.64767260969003\n",
      "Epoch [100/1000], Loss: 254.1682051254565\n",
      "Epoch [200/1000], Loss: 250.8880345184246\n",
      "Epoch [300/1000], Loss: 250.30935966980442\n",
      "Epoch [400/1000], Loss: 250.17839404300423\n",
      "Epoch [500/1000], Loss: 250.1466403128518\n",
      "Epoch [600/1000], Loss: 250.13829945634075\n",
      "Epoch [700/1000], Loss: 250.1360001123283\n",
      "Epoch [800/1000], Loss: 250.13534966305735\n",
      "Epoch [900/1000], Loss: 250.13517059630044\n",
      "Epoch [1000/1000], Loss: 250.13513164142609\n",
      "Epoch [100/1000], Loss: 253.3136260956282\n",
      "Epoch [200/1000], Loss: 250.0090363233449\n",
      "Epoch [300/1000], Loss: 249.4090864038126\n",
      "Epoch [400/1000], Loss: 249.2715114661782\n",
      "Epoch [500/1000], Loss: 249.23753052841727\n",
      "Epoch [600/1000], Loss: 249.22812947812264\n",
      "Epoch [700/1000], Loss: 249.22537550712795\n",
      "Epoch [800/1000], Loss: 249.22445035625094\n",
      "Epoch [900/1000], Loss: 249.224782048478\n",
      "Epoch [1000/1000], Loss: 249.22453438144396\n",
      "Epoch [100/1000], Loss: 254.6324425330558\n",
      "Epoch [200/1000], Loss: 251.47187094122154\n",
      "Epoch [300/1000], Loss: 250.91172295681895\n",
      "Epoch [400/1000], Loss: 250.78490319129386\n",
      "Epoch [500/1000], Loss: 250.75422269225254\n",
      "Epoch [600/1000], Loss: 250.7459179468906\n",
      "Epoch [700/1000], Loss: 250.74356753895785\n",
      "Epoch [800/1000], Loss: 250.7428374585898\n",
      "Epoch [900/1000], Loss: 250.7425683712334\n",
      "Epoch [1000/1000], Loss: 250.7424408582231\n",
      "Epoch [100/1000], Loss: 365.61437934037474\n",
      "Epoch [200/1000], Loss: 364.8625197896458\n",
      "Epoch [300/1000], Loss: 364.81981665778153\n",
      "Epoch [400/1000], Loss: 364.81140567330294\n",
      "Epoch [500/1000], Loss: 364.81167068122033\n",
      "Epoch [600/1000], Loss: 364.8123098084548\n",
      "Epoch [700/1000], Loss: 364.8131345816978\n",
      "Epoch [800/1000], Loss: 364.8087109992389\n",
      "Epoch [900/1000], Loss: 364.81429784601517\n",
      "Epoch [1000/1000], Loss: 364.81500604096414\n",
      "Epoch [100/1000], Loss: 359.98531612896056\n",
      "Epoch [200/1000], Loss: 359.27096864591454\n",
      "Epoch [300/1000], Loss: 359.2271277122041\n",
      "Epoch [400/1000], Loss: 359.22568158469113\n",
      "Epoch [500/1000], Loss: 359.2215132969941\n",
      "Epoch [600/1000], Loss: 359.224015055671\n",
      "Epoch [700/1000], Loss: 359.2202842615061\n",
      "Epoch [800/1000], Loss: 359.22280074449213\n",
      "Epoch [900/1000], Loss: 359.2213462662388\n",
      "Epoch [1000/1000], Loss: 359.22159268886026\n",
      "Epoch [100/1000], Loss: 362.9489643230802\n",
      "Epoch [200/1000], Loss: 362.21201516723886\n",
      "Epoch [300/1000], Loss: 362.16800127574686\n",
      "Epoch [400/1000], Loss: 362.16164837299647\n",
      "Epoch [500/1000], Loss: 362.16447760634924\n",
      "Epoch [600/1000], Loss: 362.1653935275218\n",
      "Epoch [700/1000], Loss: 362.1626505533031\n",
      "Epoch [800/1000], Loss: 362.163362365489\n",
      "Epoch [900/1000], Loss: 362.16733249366087\n",
      "Epoch [1000/1000], Loss: 362.16454752853724\n",
      "Epoch [100/1000], Loss: 362.4268390506162\n",
      "Epoch [200/1000], Loss: 361.62707344510056\n",
      "Epoch [300/1000], Loss: 361.57497354330724\n",
      "Epoch [400/1000], Loss: 361.5714422811451\n",
      "Epoch [500/1000], Loss: 361.5721777345333\n",
      "Epoch [600/1000], Loss: 361.5733769561333\n",
      "Epoch [700/1000], Loss: 361.57462877016826\n",
      "Epoch [800/1000], Loss: 361.57063128768823\n",
      "Epoch [900/1000], Loss: 361.5717905264936\n",
      "Epoch [1000/1000], Loss: 361.5730028411851\n",
      "Epoch [100/1000], Loss: 358.7210308355817\n",
      "Epoch [200/1000], Loss: 358.00919361447774\n",
      "Epoch [300/1000], Loss: 357.9596944338373\n",
      "Epoch [400/1000], Loss: 357.95884099776674\n",
      "Epoch [500/1000], Loss: 357.9572410123865\n",
      "Epoch [600/1000], Loss: 357.95870228948905\n",
      "Epoch [700/1000], Loss: 357.9596428599645\n",
      "Epoch [800/1000], Loss: 357.9550827536815\n",
      "Epoch [900/1000], Loss: 357.95643548032484\n",
      "Epoch [1000/1000], Loss: 357.95734864069505\n",
      "Epoch [100/1000], Loss: 356.25652375946277\n",
      "Epoch [200/1000], Loss: 355.5458762744951\n",
      "Epoch [300/1000], Loss: 355.4982485433237\n",
      "Epoch [400/1000], Loss: 355.4994921252857\n",
      "Epoch [500/1000], Loss: 355.4977823411433\n",
      "Epoch [600/1000], Loss: 355.49448538104457\n",
      "Epoch [700/1000], Loss: 355.49639493882194\n",
      "Epoch [800/1000], Loss: 355.49891963193033\n",
      "Epoch [900/1000], Loss: 355.49571301775813\n",
      "Epoch [1000/1000], Loss: 355.4965619891699\n",
      "Epoch [100/1000], Loss: 363.76145061517127\n",
      "Epoch [200/1000], Loss: 362.9911911929956\n",
      "Epoch [300/1000], Loss: 362.9458766646529\n",
      "Epoch [400/1000], Loss: 362.93962917858966\n",
      "Epoch [500/1000], Loss: 362.9438051526247\n",
      "Epoch [600/1000], Loss: 362.9394578312876\n",
      "Epoch [700/1000], Loss: 362.9414633212171\n",
      "Epoch [800/1000], Loss: 362.9418152181323\n",
      "Epoch [900/1000], Loss: 362.9437007264404\n",
      "Epoch [1000/1000], Loss: 362.9395034885614\n",
      "Epoch [100/1000], Loss: 360.07926663390236\n",
      "Epoch [200/1000], Loss: 359.3168042699493\n",
      "Epoch [300/1000], Loss: 359.27683984809676\n",
      "Epoch [400/1000], Loss: 359.2719257652311\n",
      "Epoch [500/1000], Loss: 359.2690207528555\n",
      "Epoch [600/1000], Loss: 359.2696084848006\n",
      "Epoch [700/1000], Loss: 359.26896596957033\n",
      "Epoch [800/1000], Loss: 359.27178445200843\n",
      "Epoch [900/1000], Loss: 359.2692968804911\n",
      "Epoch [1000/1000], Loss: 359.27178911101635\n",
      "Epoch [100/1000], Loss: 358.99717771005345\n",
      "Epoch [200/1000], Loss: 358.23302586172497\n",
      "Epoch [300/1000], Loss: 358.1887682559069\n",
      "Epoch [400/1000], Loss: 358.18303183712624\n",
      "Epoch [500/1000], Loss: 358.18618834523835\n",
      "Epoch [600/1000], Loss: 358.1833429553123\n",
      "Epoch [700/1000], Loss: 358.17923063796826\n",
      "Epoch [800/1000], Loss: 358.1804917481006\n",
      "Epoch [900/1000], Loss: 358.17902200598905\n",
      "Epoch [1000/1000], Loss: 358.1825643479381\n",
      "Epoch [100/1000], Loss: 362.7213202299879\n",
      "Epoch [200/1000], Loss: 362.0035738365456\n",
      "Epoch [300/1000], Loss: 361.96340847025294\n",
      "Epoch [400/1000], Loss: 361.95917300308116\n",
      "Epoch [500/1000], Loss: 361.95983186940407\n",
      "Epoch [600/1000], Loss: 361.9608051445309\n",
      "Epoch [700/1000], Loss: 361.9563811135604\n",
      "Epoch [800/1000], Loss: 361.9574883650172\n",
      "Epoch [900/1000], Loss: 361.9586627804018\n",
      "Epoch [1000/1000], Loss: 361.95952164047753\n",
      "Epoch [100/1000], Loss: 446.8541684458153\n",
      "Epoch [200/1000], Loss: 446.6196909618018\n",
      "Epoch [300/1000], Loss: 446.6028476378275\n",
      "Epoch [400/1000], Loss: 446.607993526398\n",
      "Epoch [500/1000], Loss: 446.6139112686477\n",
      "Epoch [600/1000], Loss: 446.6033707615701\n",
      "Epoch [700/1000], Loss: 446.6090226214542\n",
      "Epoch [800/1000], Loss: 446.61339457688416\n",
      "Epoch [900/1000], Loss: 446.6042111524695\n",
      "Epoch [1000/1000], Loss: 446.6099148078172\n",
      "Epoch [100/1000], Loss: 439.1346369272673\n",
      "Epoch [200/1000], Loss: 438.907470897031\n",
      "Epoch [300/1000], Loss: 438.8998314142834\n",
      "Epoch [400/1000], Loss: 438.8992323698666\n",
      "Epoch [500/1000], Loss: 438.89913057802477\n",
      "Epoch [600/1000], Loss: 438.8991061332112\n",
      "Epoch [700/1000], Loss: 438.89909983456545\n",
      "Epoch [800/1000], Loss: 438.89909822323057\n",
      "Epoch [900/1000], Loss: 438.8990978201086\n",
      "Epoch [1000/1000], Loss: 438.89909772146757\n",
      "Epoch [100/1000], Loss: 443.48551517249626\n",
      "Epoch [200/1000], Loss: 443.25819556833767\n",
      "Epoch [300/1000], Loss: 443.2501350999486\n",
      "Epoch [400/1000], Loss: 443.26290208092274\n",
      "Epoch [500/1000], Loss: 443.26426710785705\n",
      "Epoch [600/1000], Loss: 443.26270549722074\n",
      "Epoch [700/1000], Loss: 443.26091308479744\n",
      "Epoch [800/1000], Loss: 443.2554170759786\n",
      "Epoch [900/1000], Loss: 443.249760791095\n",
      "Epoch [1000/1000], Loss: 443.25720789813056\n",
      "Epoch [100/1000], Loss: 441.75890000282266\n",
      "Epoch [200/1000], Loss: 441.5304163409163\n",
      "Epoch [300/1000], Loss: 441.51549254581795\n",
      "Epoch [400/1000], Loss: 441.52319725967027\n",
      "Epoch [500/1000], Loss: 441.51327004572113\n",
      "Epoch [600/1000], Loss: 441.5209630987364\n",
      "Epoch [700/1000], Loss: 441.5128105837597\n",
      "Epoch [800/1000], Loss: 441.5208410794097\n",
      "Epoch [900/1000], Loss: 441.5248620295601\n",
      "Epoch [1000/1000], Loss: 441.5180413022615\n",
      "Epoch [100/1000], Loss: 438.1325169798894\n",
      "Epoch [200/1000], Loss: 437.9045962927471\n",
      "Epoch [300/1000], Loss: 437.9044026456688\n",
      "Epoch [400/1000], Loss: 437.90972672540005\n",
      "Epoch [500/1000], Loss: 437.915140265005\n",
      "Epoch [600/1000], Loss: 437.91897083250717\n",
      "Epoch [700/1000], Loss: 437.9094301955233\n",
      "Epoch [800/1000], Loss: 437.9049226877802\n",
      "Epoch [900/1000], Loss: 437.9008527715233\n",
      "Epoch [1000/1000], Loss: 437.90609350000165\n",
      "Epoch [100/1000], Loss: 434.30949524700776\n",
      "Epoch [200/1000], Loss: 434.08472475363794\n",
      "Epoch [300/1000], Loss: 434.0809371965606\n",
      "Epoch [400/1000], Loss: 434.08334644753364\n",
      "Epoch [500/1000], Loss: 434.086325663037\n",
      "Epoch [600/1000], Loss: 434.0891146268242\n",
      "Epoch [700/1000], Loss: 434.08518212623045\n",
      "Epoch [800/1000], Loss: 434.0772725149541\n",
      "Epoch [900/1000], Loss: 434.07997943017375\n",
      "Epoch [1000/1000], Loss: 434.0827081277547\n",
      "Epoch [100/1000], Loss: 443.43376718011103\n",
      "Epoch [200/1000], Loss: 443.1921989488552\n",
      "Epoch [300/1000], Loss: 443.1869633606106\n",
      "Epoch [400/1000], Loss: 443.19400273218486\n",
      "Epoch [500/1000], Loss: 443.20226624581915\n",
      "Epoch [600/1000], Loss: 443.19569007724436\n",
      "Epoch [700/1000], Loss: 443.18945164547745\n",
      "Epoch [800/1000], Loss: 443.1892586302839\n",
      "Epoch [900/1000], Loss: 443.19486403800425\n",
      "Epoch [1000/1000], Loss: 443.1879671761443\n",
      "Epoch [100/1000], Loss: 438.9879971809813\n",
      "Epoch [200/1000], Loss: 438.75484546240284\n",
      "Epoch [300/1000], Loss: 438.75069250408956\n",
      "Epoch [400/1000], Loss: 438.75045749042795\n",
      "Epoch [500/1000], Loss: 438.7503107936261\n",
      "Epoch [600/1000], Loss: 438.75006870043944\n",
      "Epoch [700/1000], Loss: 438.74976895428404\n",
      "Epoch [800/1000], Loss: 438.74941282339535\n",
      "Epoch [900/1000], Loss: 438.7492444541316\n",
      "Epoch [1000/1000], Loss: 438.7502110894228\n",
      "Epoch [100/1000], Loss: 437.68067598092864\n",
      "Epoch [200/1000], Loss: 437.4485924088183\n",
      "Epoch [300/1000], Loss: 437.4301933553102\n",
      "Epoch [400/1000], Loss: 437.43995515877145\n",
      "Epoch [500/1000], Loss: 437.44106902668534\n",
      "Epoch [600/1000], Loss: 437.44602963247206\n",
      "Epoch [700/1000], Loss: 437.43355768230185\n",
      "Epoch [800/1000], Loss: 437.4386110537315\n",
      "Epoch [900/1000], Loss: 437.43520306287724\n",
      "Epoch [1000/1000], Loss: 437.4496478918685\n",
      "Epoch [100/1000], Loss: 443.3440676427232\n",
      "Epoch [200/1000], Loss: 443.1097484345438\n",
      "Epoch [300/1000], Loss: 443.1058586369645\n",
      "Epoch [400/1000], Loss: 443.0997125485816\n",
      "Epoch [500/1000], Loss: 443.10767468632247\n",
      "Epoch [600/1000], Loss: 443.10982347612514\n",
      "Epoch [700/1000], Loss: 443.09681813345776\n",
      "Epoch [800/1000], Loss: 443.10432021082977\n",
      "Epoch [900/1000], Loss: 443.113883167861\n",
      "Epoch [1000/1000], Loss: 443.0972301782326\n",
      "Epoch [100/1000], Loss: 509.31513606133825\n",
      "Epoch [200/1000], Loss: 509.2250277518694\n",
      "Epoch [300/1000], Loss: 509.22537539576217\n",
      "Epoch [400/1000], Loss: 509.2227388327896\n",
      "Epoch [500/1000], Loss: 509.2272837977534\n",
      "Epoch [600/1000], Loss: 509.22948388212393\n",
      "Epoch [700/1000], Loss: 509.22660223968535\n",
      "Epoch [800/1000], Loss: 509.22324593470546\n",
      "Epoch [900/1000], Loss: 509.23346653356896\n",
      "Epoch [1000/1000], Loss: 509.2304321319056\n",
      "Epoch [100/1000], Loss: 499.9383741149173\n",
      "Epoch [200/1000], Loss: 499.85012927577094\n",
      "Epoch [300/1000], Loss: 499.86944156218124\n",
      "Epoch [400/1000], Loss: 499.84761302660604\n",
      "Epoch [500/1000], Loss: 499.8469806191638\n",
      "Epoch [600/1000], Loss: 499.8474570031782\n",
      "Epoch [700/1000], Loss: 499.87250477343036\n",
      "Epoch [800/1000], Loss: 499.8463552896093\n",
      "Epoch [900/1000], Loss: 499.8459634169706\n",
      "Epoch [1000/1000], Loss: 499.8466695567623\n",
      "Epoch [100/1000], Loss: 505.4127470913905\n",
      "Epoch [200/1000], Loss: 505.352366192589\n",
      "Epoch [300/1000], Loss: 505.33491341099125\n",
      "Epoch [400/1000], Loss: 505.35370663758476\n",
      "Epoch [500/1000], Loss: 505.3361462403022\n",
      "Epoch [600/1000], Loss: 505.3346028424735\n",
      "Epoch [700/1000], Loss: 505.3371643699147\n",
      "Epoch [800/1000], Loss: 505.3211488501107\n",
      "Epoch [900/1000], Loss: 505.3397590514681\n",
      "Epoch [1000/1000], Loss: 505.32309491677853\n",
      "Epoch [100/1000], Loss: 502.7232170133742\n",
      "Epoch [200/1000], Loss: 502.6241753659474\n",
      "Epoch [300/1000], Loss: 502.64215511946406\n",
      "Epoch [400/1000], Loss: 502.64295543864125\n",
      "Epoch [500/1000], Loss: 502.64234167881324\n",
      "Epoch [600/1000], Loss: 502.64539414067343\n",
      "Epoch [700/1000], Loss: 502.63294769492757\n",
      "Epoch [800/1000], Loss: 502.64454758586356\n",
      "Epoch [900/1000], Loss: 502.6142092970098\n",
      "Epoch [1000/1000], Loss: 502.6452835956772\n",
      "Epoch [100/1000], Loss: 499.1712129615787\n",
      "Epoch [200/1000], Loss: 499.0851869098208\n",
      "Epoch [300/1000], Loss: 499.0815310267225\n",
      "Epoch [400/1000], Loss: 499.08024010093953\n",
      "Epoch [500/1000], Loss: 499.10750157627575\n",
      "Epoch [600/1000], Loss: 499.08203050670227\n",
      "Epoch [700/1000], Loss: 499.093612140384\n",
      "Epoch [800/1000], Loss: 499.0779700991979\n",
      "Epoch [900/1000], Loss: 499.09663227423664\n",
      "Epoch [1000/1000], Loss: 499.09925172621433\n",
      "Epoch [100/1000], Loss: 494.29450151118317\n",
      "Epoch [200/1000], Loss: 494.2035832266307\n",
      "Epoch [300/1000], Loss: 494.20472278014137\n",
      "Epoch [400/1000], Loss: 494.22390907484123\n",
      "Epoch [500/1000], Loss: 494.2154597241086\n",
      "Epoch [600/1000], Loss: 494.20732474036015\n",
      "Epoch [700/1000], Loss: 494.19951289451143\n",
      "Epoch [800/1000], Loss: 494.21847553028437\n",
      "Epoch [900/1000], Loss: 494.2198588826015\n",
      "Epoch [1000/1000], Loss: 494.21154201393347\n",
      "Epoch [100/1000], Loss: 504.63337689159886\n",
      "Epoch [200/1000], Loss: 504.5860382497704\n",
      "Epoch [300/1000], Loss: 504.5610926079215\n",
      "Epoch [400/1000], Loss: 504.5728221405262\n",
      "Epoch [500/1000], Loss: 504.5474741726851\n",
      "Epoch [600/1000], Loss: 504.5595665474039\n",
      "Epoch [700/1000], Loss: 504.54191515068715\n",
      "Epoch [800/1000], Loss: 504.5557423769297\n",
      "Epoch [900/1000], Loss: 504.56900506645394\n",
      "Epoch [1000/1000], Loss: 504.57363817336636\n",
      "Epoch [100/1000], Loss: 499.65596026086325\n",
      "Epoch [200/1000], Loss: 499.5497560289144\n",
      "Epoch [300/1000], Loss: 499.5598528487824\n",
      "Epoch [400/1000], Loss: 499.5610215155427\n",
      "Epoch [500/1000], Loss: 499.55676736593097\n",
      "Epoch [600/1000], Loss: 499.57030120141496\n",
      "Epoch [700/1000], Loss: 499.55401415100164\n",
      "Epoch [800/1000], Loss: 499.5708114937206\n",
      "Epoch [900/1000], Loss: 499.5515452196358\n",
      "Epoch [1000/1000], Loss: 499.5680030261086\n",
      "Epoch [100/1000], Loss: 498.09810630920197\n",
      "Epoch [200/1000], Loss: 498.0171357679186\n",
      "Epoch [300/1000], Loss: 498.02077641247195\n",
      "Epoch [400/1000], Loss: 498.026433780905\n",
      "Epoch [500/1000], Loss: 498.03195258389735\n",
      "Epoch [600/1000], Loss: 498.01642346178306\n",
      "Epoch [700/1000], Loss: 498.0051210380052\n",
      "Epoch [800/1000], Loss: 498.0104849045767\n",
      "Epoch [900/1000], Loss: 498.0157175259837\n",
      "Epoch [1000/1000], Loss: 498.0207721663872\n",
      "Epoch [100/1000], Loss: 505.33783706362937\n",
      "Epoch [200/1000], Loss: 505.2406854656933\n",
      "Epoch [300/1000], Loss: 505.22124059933543\n",
      "Epoch [400/1000], Loss: 505.2424968699792\n",
      "Epoch [500/1000], Loss: 505.26728382085673\n",
      "Epoch [600/1000], Loss: 505.24482387393607\n",
      "Epoch [700/1000], Loss: 505.2370607556436\n",
      "Epoch [800/1000], Loss: 505.2406137353855\n",
      "Epoch [900/1000], Loss: 505.24179489437756\n",
      "Epoch [1000/1000], Loss: 505.2292245722916\n",
      "Epoch [100/1000], Loss: 31.80118838384418\n",
      "Epoch [200/1000], Loss: 24.517642024858013\n",
      "Epoch [300/1000], Loss: 18.820011028599474\n",
      "Epoch [400/1000], Loss: 15.474623605695664\n",
      "Epoch [500/1000], Loss: 14.008419732899355\n",
      "Epoch [600/1000], Loss: 13.274458006629407\n",
      "Epoch [700/1000], Loss: 12.769365964123105\n",
      "Epoch [800/1000], Loss: 12.375500446354474\n",
      "Epoch [900/1000], Loss: 12.053401686286325\n",
      "Epoch [1000/1000], Loss: 11.780816154600869\n",
      "Epoch [100/1000], Loss: 31.64475140119601\n",
      "Epoch [200/1000], Loss: 24.399353109241986\n",
      "Epoch [300/1000], Loss: 18.7441132476843\n",
      "Epoch [400/1000], Loss: 15.433279279723893\n",
      "Epoch [500/1000], Loss: 13.980025781453678\n",
      "Epoch [600/1000], Loss: 13.247885073033059\n",
      "Epoch [700/1000], Loss: 12.7431499489388\n",
      "Epoch [800/1000], Loss: 12.34983270141444\n",
      "Epoch [900/1000], Loss: 12.028357393979592\n",
      "Epoch [1000/1000], Loss: 11.756523547617308\n",
      "Epoch [100/1000], Loss: 31.717892450886804\n",
      "Epoch [200/1000], Loss: 24.41424583157328\n",
      "Epoch [300/1000], Loss: 18.68873059693949\n",
      "Epoch [400/1000], Loss: 15.320197964721556\n",
      "Epoch [500/1000], Loss: 13.842210855241062\n",
      "Epoch [600/1000], Loss: 13.102930738487212\n",
      "Epoch [700/1000], Loss: 12.598729405836895\n",
      "Epoch [800/1000], Loss: 12.210104875160837\n",
      "Epoch [900/1000], Loss: 11.894956458688629\n",
      "Epoch [1000/1000], Loss: 11.629452670447142\n",
      "Epoch [100/1000], Loss: 31.46063096108906\n",
      "Epoch [200/1000], Loss: 24.254359870980974\n",
      "Epoch [300/1000], Loss: 18.669055835988072\n",
      "Epoch [400/1000], Loss: 15.433932040772438\n",
      "Epoch [500/1000], Loss: 14.01563885889503\n",
      "Epoch [600/1000], Loss: 13.290903291446678\n",
      "Epoch [700/1000], Loss: 12.786754827013066\n",
      "Epoch [800/1000], Loss: 12.39236446994325\n",
      "Epoch [900/1000], Loss: 12.069164943621264\n",
      "Epoch [1000/1000], Loss: 11.795339224149805\n",
      "Epoch [100/1000], Loss: 31.5638015883403\n",
      "Epoch [200/1000], Loss: 24.275751716687708\n",
      "Epoch [300/1000], Loss: 18.567789656934444\n",
      "Epoch [400/1000], Loss: 15.22815078167371\n",
      "Epoch [500/1000], Loss: 13.782698827667057\n",
      "Epoch [600/1000], Loss: 13.064049487703585\n",
      "Epoch [700/1000], Loss: 12.567956677952214\n",
      "Epoch [800/1000], Loss: 12.179986064693727\n",
      "Epoch [900/1000], Loss: 11.862155318987437\n",
      "Epoch [1000/1000], Loss: 11.592861756209885\n",
      "Epoch [100/1000], Loss: 31.222657769735633\n",
      "Epoch [200/1000], Loss: 23.999884673356306\n",
      "Epoch [300/1000], Loss: 18.42489322074916\n",
      "Epoch [400/1000], Loss: 15.241935903247263\n",
      "Epoch [500/1000], Loss: 13.877841519669706\n",
      "Epoch [600/1000], Loss: 13.184159985836134\n",
      "Epoch [700/1000], Loss: 12.6964858837539\n",
      "Epoch [800/1000], Loss: 12.311354414780062\n",
      "Epoch [900/1000], Loss: 11.993773634385933\n",
      "Epoch [1000/1000], Loss: 11.723846235134548\n",
      "Epoch [100/1000], Loss: 31.649502479666463\n",
      "Epoch [200/1000], Loss: 24.44269721077591\n",
      "Epoch [300/1000], Loss: 18.838026323992228\n",
      "Epoch [400/1000], Loss: 15.557947207362291\n",
      "Epoch [500/1000], Loss: 14.097465173946617\n",
      "Epoch [600/1000], Loss: 13.349168143165274\n",
      "Epoch [700/1000], Loss: 12.833141621774407\n",
      "Epoch [800/1000], Loss: 12.431999089318053\n",
      "Epoch [900/1000], Loss: 12.103409751773638\n",
      "Epoch [1000/1000], Loss: 11.823732808979623\n",
      "Epoch [100/1000], Loss: 31.221248968959834\n",
      "Epoch [200/1000], Loss: 24.046014625327985\n",
      "Epoch [300/1000], Loss: 18.54833699414496\n",
      "Epoch [400/1000], Loss: 15.415235893214085\n",
      "Epoch [500/1000], Loss: 14.033806116582666\n",
      "Epoch [600/1000], Loss: 13.306745836238163\n",
      "Epoch [700/1000], Loss: 12.798436985770923\n",
      "Epoch [800/1000], Loss: 12.40504204798042\n",
      "Epoch [900/1000], Loss: 12.086603911786753\n",
      "Epoch [1000/1000], Loss: 11.819496652269061\n",
      "Epoch [100/1000], Loss: 31.340777809268946\n",
      "Epoch [200/1000], Loss: 24.116766413291025\n",
      "Epoch [300/1000], Loss: 18.52495535947818\n",
      "Epoch [400/1000], Loss: 15.296622395560076\n",
      "Epoch [500/1000], Loss: 13.879217219245277\n",
      "Epoch [600/1000], Loss: 13.149207231620997\n",
      "Epoch [700/1000], Loss: 12.643007112187007\n",
      "Epoch [800/1000], Loss: 12.250055095357437\n",
      "Epoch [900/1000], Loss: 11.929396549925855\n",
      "Epoch [1000/1000], Loss: 11.657771210317906\n",
      "Epoch [100/1000], Loss: 31.911609693272677\n",
      "Epoch [200/1000], Loss: 24.583854885169483\n",
      "Epoch [300/1000], Loss: 18.810665664804347\n",
      "Epoch [400/1000], Loss: 15.387793932527345\n",
      "Epoch [500/1000], Loss: 13.888082471197269\n",
      "Epoch [600/1000], Loss: 13.14757562612467\n",
      "Epoch [700/1000], Loss: 12.642764003781803\n",
      "Epoch [800/1000], Loss: 12.251715091183062\n",
      "Epoch [900/1000], Loss: 11.934052542903775\n",
      "Epoch [1000/1000], Loss: 11.666956795184525\n",
      "Epoch [100/1000], Loss: 32.24591922437558\n",
      "Epoch [200/1000], Loss: 25.269612427136295\n",
      "Epoch [300/1000], Loss: 19.823368563699063\n",
      "Epoch [400/1000], Loss: 16.637627935310363\n",
      "Epoch [500/1000], Loss: 15.242403419736116\n",
      "Epoch [600/1000], Loss: 14.543916353498012\n",
      "Epoch [700/1000], Loss: 14.071993193736802\n",
      "Epoch [800/1000], Loss: 13.71724026166704\n",
      "Epoch [900/1000], Loss: 13.431723495198924\n",
      "Epoch [1000/1000], Loss: 13.18946503971673\n",
      "Epoch [100/1000], Loss: 31.857584516957818\n",
      "Epoch [200/1000], Loss: 24.961182001801163\n",
      "Epoch [300/1000], Loss: 19.63562052606404\n",
      "Epoch [400/1000], Loss: 16.56688406377227\n",
      "Epoch [500/1000], Loss: 15.21581638545382\n",
      "Epoch [600/1000], Loss: 14.519943317405474\n",
      "Epoch [700/1000], Loss: 14.04474077244788\n",
      "Epoch [800/1000], Loss: 13.690573065426697\n",
      "Epoch [900/1000], Loss: 13.403483020378323\n",
      "Epoch [1000/1000], Loss: 13.15984943034459\n",
      "Epoch [100/1000], Loss: 32.138594517657474\n",
      "Epoch [200/1000], Loss: 25.169045522184376\n",
      "Epoch [300/1000], Loss: 19.735267050816486\n",
      "Epoch [400/1000], Loss: 16.555893332922732\n",
      "Epoch [500/1000], Loss: 15.147174989248988\n",
      "Epoch [600/1000], Loss: 14.432022993983733\n",
      "Epoch [700/1000], Loss: 13.952553995062429\n",
      "Epoch [800/1000], Loss: 13.60060258623193\n",
      "Epoch [900/1000], Loss: 13.31767564997623\n",
      "Epoch [1000/1000], Loss: 13.078711425774388\n",
      "Epoch [100/1000], Loss: 31.734684501735558\n",
      "Epoch [200/1000], Loss: 24.868824265672714\n",
      "Epoch [300/1000], Loss: 19.595252917697877\n",
      "Epoch [400/1000], Loss: 16.583902685900142\n",
      "Epoch [500/1000], Loss: 15.265497139941086\n",
      "Epoch [600/1000], Loss: 14.583010866541896\n",
      "Epoch [700/1000], Loss: 14.113096666494675\n",
      "Epoch [800/1000], Loss: 13.754839906795116\n",
      "Epoch [900/1000], Loss: 13.467569523987013\n",
      "Epoch [1000/1000], Loss: 13.225237582902183\n",
      "Epoch [100/1000], Loss: 31.60331696092132\n",
      "Epoch [200/1000], Loss: 24.69621632796097\n",
      "Epoch [300/1000], Loss: 19.37766571867173\n",
      "Epoch [400/1000], Loss: 16.347915114653627\n",
      "Epoch [500/1000], Loss: 15.037610076815074\n",
      "Epoch [600/1000], Loss: 14.363427502583377\n",
      "Epoch [700/1000], Loss: 13.89844827214816\n",
      "Epoch [800/1000], Loss: 13.550570237814265\n",
      "Epoch [900/1000], Loss: 13.2633349027441\n",
      "Epoch [1000/1000], Loss: 13.01819163157785\n",
      "Epoch [100/1000], Loss: 31.685722887446616\n",
      "Epoch [200/1000], Loss: 24.765306531482455\n",
      "Epoch [300/1000], Loss: 19.426236573437095\n",
      "Epoch [400/1000], Loss: 16.381075972131818\n",
      "Epoch [500/1000], Loss: 15.073547316927945\n",
      "Epoch [600/1000], Loss: 14.410711540614514\n",
      "Epoch [700/1000], Loss: 13.956630777455878\n",
      "Epoch [800/1000], Loss: 13.615587071046749\n",
      "Epoch [900/1000], Loss: 13.339331937319619\n",
      "Epoch [1000/1000], Loss: 13.104102465268971\n",
      "Epoch [100/1000], Loss: 32.13461595907827\n",
      "Epoch [200/1000], Loss: 25.2300160831296\n",
      "Epoch [300/1000], Loss: 19.86627201483208\n",
      "Epoch [400/1000], Loss: 16.732555349644475\n",
      "Epoch [500/1000], Loss: 15.335126564832299\n",
      "Epoch [600/1000], Loss: 14.619678579435282\n",
      "Epoch [700/1000], Loss: 14.135771044158172\n",
      "Epoch [800/1000], Loss: 13.7719710651382\n",
      "Epoch [900/1000], Loss: 13.48007717752975\n",
      "Epoch [1000/1000], Loss: 13.230890987927491\n",
      "Epoch [100/1000], Loss: 31.893459470584457\n",
      "Epoch [200/1000], Loss: 25.003762064567002\n",
      "Epoch [300/1000], Loss: 19.698286535333075\n",
      "Epoch [400/1000], Loss: 16.64907250647404\n",
      "Epoch [500/1000], Loss: 15.298468287708415\n",
      "Epoch [600/1000], Loss: 14.595147277747547\n",
      "Epoch [700/1000], Loss: 14.114751389777362\n",
      "Epoch [800/1000], Loss: 13.759023354888114\n",
      "Epoch [900/1000], Loss: 13.472470973670033\n",
      "Epoch [1000/1000], Loss: 13.23189921693129\n",
      "Epoch [100/1000], Loss: 31.951661519016294\n",
      "Epoch [200/1000], Loss: 25.023883893079773\n",
      "Epoch [300/1000], Loss: 19.64558349978836\n",
      "Epoch [400/1000], Loss: 16.52171085290804\n",
      "Epoch [500/1000], Loss: 15.141368013913478\n",
      "Epoch [600/1000], Loss: 14.436454114625288\n",
      "Epoch [700/1000], Loss: 13.961786205892201\n",
      "Epoch [800/1000], Loss: 13.610561663954043\n",
      "Epoch [900/1000], Loss: 13.328209610362382\n",
      "Epoch [1000/1000], Loss: 13.088667722314245\n",
      "Epoch [100/1000], Loss: 32.36437746868917\n",
      "Epoch [200/1000], Loss: 25.358987233834846\n",
      "Epoch [300/1000], Loss: 19.845703640933092\n",
      "Epoch [400/1000], Loss: 16.577721505340122\n",
      "Epoch [500/1000], Loss: 15.133566968344631\n",
      "Epoch [600/1000], Loss: 14.415743454065518\n",
      "Epoch [700/1000], Loss: 13.937692854366729\n",
      "Epoch [800/1000], Loss: 13.587287951378348\n",
      "Epoch [900/1000], Loss: 13.306693978810717\n",
      "Epoch [1000/1000], Loss: 13.072587208194324\n",
      "Epoch [100/1000], Loss: 35.70933903452796\n",
      "Epoch [200/1000], Loss: 31.786037955936383\n",
      "Epoch [300/1000], Loss: 28.698477047983054\n",
      "Epoch [400/1000], Loss: 26.760135871320127\n",
      "Epoch [500/1000], Loss: 25.71350262235918\n",
      "Epoch [600/1000], Loss: 25.05453200343355\n",
      "Epoch [700/1000], Loss: 24.59186875093875\n",
      "Epoch [800/1000], Loss: 24.28005174727129\n",
      "Epoch [900/1000], Loss: 24.053087350518332\n",
      "Epoch [1000/1000], Loss: 23.889026547038736\n",
      "Epoch [100/1000], Loss: 35.24176942877062\n",
      "Epoch [200/1000], Loss: 31.39766084890899\n",
      "Epoch [300/1000], Loss: 28.38878436488962\n",
      "Epoch [400/1000], Loss: 26.506993330598092\n",
      "Epoch [500/1000], Loss: 25.51980001851146\n",
      "Epoch [600/1000], Loss: 24.91524351525176\n",
      "Epoch [700/1000], Loss: 24.50204296652612\n",
      "Epoch [800/1000], Loss: 24.21486917003091\n",
      "Epoch [900/1000], Loss: 23.997762997296547\n",
      "Epoch [1000/1000], Loss: 23.836709163647726\n",
      "Epoch [100/1000], Loss: 35.6895633130199\n",
      "Epoch [200/1000], Loss: 31.788972372045727\n",
      "Epoch [300/1000], Loss: 28.66462149592052\n",
      "Epoch [400/1000], Loss: 26.6225162384157\n",
      "Epoch [500/1000], Loss: 25.5509481273104\n",
      "Epoch [600/1000], Loss: 24.922452731102545\n",
      "Epoch [700/1000], Loss: 24.533485574093795\n",
      "Epoch [800/1000], Loss: 24.252481083921488\n",
      "Epoch [900/1000], Loss: 24.031924290173006\n",
      "Epoch [1000/1000], Loss: 23.868367243426214\n",
      "Epoch [100/1000], Loss: 35.14534354051827\n",
      "Epoch [200/1000], Loss: 31.319173913657842\n",
      "Epoch [300/1000], Loss: 28.37006968537678\n",
      "Epoch [400/1000], Loss: 26.541486212013822\n",
      "Epoch [500/1000], Loss: 25.552770446349832\n",
      "Epoch [600/1000], Loss: 24.945869083245668\n",
      "Epoch [700/1000], Loss: 24.559966478794177\n",
      "Epoch [800/1000], Loss: 24.28718595074707\n",
      "Epoch [900/1000], Loss: 24.07892027620479\n",
      "Epoch [1000/1000], Loss: 23.92842304473961\n",
      "Epoch [100/1000], Loss: 35.17876277403705\n",
      "Epoch [200/1000], Loss: 31.301269722093313\n",
      "Epoch [300/1000], Loss: 28.28574743696173\n",
      "Epoch [400/1000], Loss: 26.413937752433306\n",
      "Epoch [500/1000], Loss: 25.4181554647365\n",
      "Epoch [600/1000], Loss: 24.799760924071435\n",
      "Epoch [700/1000], Loss: 24.370651377725522\n",
      "Epoch [800/1000], Loss: 24.085824963750827\n",
      "Epoch [900/1000], Loss: 23.874513863023303\n",
      "Epoch [1000/1000], Loss: 23.714069507286347\n",
      "Epoch [100/1000], Loss: 34.89884413159886\n",
      "Epoch [200/1000], Loss: 31.05506232506186\n",
      "Epoch [300/1000], Loss: 28.15742325302454\n",
      "Epoch [400/1000], Loss: 26.40269801478049\n",
      "Epoch [500/1000], Loss: 25.481180956472166\n",
      "Epoch [600/1000], Loss: 24.870185194601255\n",
      "Epoch [700/1000], Loss: 24.452528349435955\n",
      "Epoch [800/1000], Loss: 24.146855743012036\n",
      "Epoch [900/1000], Loss: 23.91045644890021\n",
      "Epoch [1000/1000], Loss: 23.74905947502384\n",
      "Epoch [100/1000], Loss: 35.34916249618822\n",
      "Epoch [200/1000], Loss: 31.52458208428323\n",
      "Epoch [300/1000], Loss: 28.553124099369445\n",
      "Epoch [400/1000], Loss: 26.67660807273333\n",
      "Epoch [500/1000], Loss: 25.693773548887613\n",
      "Epoch [600/1000], Loss: 25.080763006685526\n",
      "Epoch [700/1000], Loss: 24.675423792833666\n",
      "Epoch [800/1000], Loss: 24.37995035841881\n",
      "Epoch [900/1000], Loss: 24.157473054773938\n",
      "Epoch [1000/1000], Loss: 23.99265875887516\n",
      "Epoch [100/1000], Loss: 35.03884830048152\n",
      "Epoch [200/1000], Loss: 31.19830924268031\n",
      "Epoch [300/1000], Loss: 28.252823709677205\n",
      "Epoch [400/1000], Loss: 26.437523754994416\n",
      "Epoch [500/1000], Loss: 25.44688414455073\n",
      "Epoch [600/1000], Loss: 24.842638560326712\n",
      "Epoch [700/1000], Loss: 24.463034311527274\n",
      "Epoch [800/1000], Loss: 24.182352958636045\n",
      "Epoch [900/1000], Loss: 23.96868381520733\n",
      "Epoch [1000/1000], Loss: 23.824615996313078\n",
      "Epoch [100/1000], Loss: 34.946620465012\n",
      "Epoch [200/1000], Loss: 31.08968178577655\n",
      "Epoch [300/1000], Loss: 28.15249194549337\n",
      "Epoch [400/1000], Loss: 26.36054146083127\n",
      "Epoch [500/1000], Loss: 25.397178076770707\n",
      "Epoch [600/1000], Loss: 24.814411719548477\n",
      "Epoch [700/1000], Loss: 24.452655224380397\n",
      "Epoch [800/1000], Loss: 24.18524939906355\n",
      "Epoch [900/1000], Loss: 23.97136773332727\n",
      "Epoch [1000/1000], Loss: 23.819186894122897\n",
      "Epoch [100/1000], Loss: 35.70348269058844\n",
      "Epoch [200/1000], Loss: 31.799495671517278\n",
      "Epoch [300/1000], Loss: 28.70742492557089\n",
      "Epoch [400/1000], Loss: 26.73876797022273\n",
      "Epoch [500/1000], Loss: 25.694565542252285\n",
      "Epoch [600/1000], Loss: 25.05239041121444\n",
      "Epoch [700/1000], Loss: 24.620724768847698\n",
      "Epoch [800/1000], Loss: 24.326961319707237\n",
      "Epoch [900/1000], Loss: 24.088553348977072\n",
      "Epoch [1000/1000], Loss: 23.896005289287558\n",
      "Epoch [100/1000], Loss: 39.93740227547787\n",
      "Epoch [200/1000], Loss: 39.9416549778851\n",
      "Epoch [300/1000], Loss: 39.933947763167176\n",
      "Epoch [400/1000], Loss: 39.93902739076793\n",
      "Epoch [500/1000], Loss: 39.945167617714674\n",
      "Epoch [600/1000], Loss: 39.940247081178384\n",
      "Epoch [700/1000], Loss: 39.93871382003234\n",
      "Epoch [800/1000], Loss: 39.94693247522116\n",
      "Epoch [900/1000], Loss: 39.937478765791425\n",
      "Epoch [1000/1000], Loss: 39.9422038370215\n",
      "Epoch [100/1000], Loss: 39.47607233959381\n",
      "Epoch [200/1000], Loss: 39.48169565240443\n",
      "Epoch [300/1000], Loss: 39.483630931248484\n",
      "Epoch [400/1000], Loss: 39.484193341229094\n",
      "Epoch [500/1000], Loss: 39.47450409989752\n",
      "Epoch [600/1000], Loss: 39.48671597630775\n",
      "Epoch [700/1000], Loss: 39.47908985896134\n",
      "Epoch [800/1000], Loss: 39.47440805370571\n",
      "Epoch [900/1000], Loss: 39.48961979225983\n",
      "Epoch [1000/1000], Loss: 39.47739285494489\n",
      "Epoch [100/1000], Loss: 39.76885406741901\n",
      "Epoch [200/1000], Loss: 39.7571942076346\n",
      "Epoch [300/1000], Loss: 39.76107488894492\n",
      "Epoch [400/1000], Loss: 39.761423442559966\n",
      "Epoch [500/1000], Loss: 39.75365110968437\n",
      "Epoch [600/1000], Loss: 39.75339768394885\n",
      "Epoch [700/1000], Loss: 39.75715818579333\n",
      "Epoch [800/1000], Loss: 39.75213978954585\n",
      "Epoch [900/1000], Loss: 39.76241621535892\n",
      "Epoch [1000/1000], Loss: 39.762793169892625\n",
      "Epoch [100/1000], Loss: 39.542091265710646\n",
      "Epoch [200/1000], Loss: 39.54077706152247\n",
      "Epoch [300/1000], Loss: 39.544127298697475\n",
      "Epoch [400/1000], Loss: 39.54320537140221\n",
      "Epoch [500/1000], Loss: 39.54096012636615\n",
      "Epoch [600/1000], Loss: 39.543308774718916\n",
      "Epoch [700/1000], Loss: 39.547153616860996\n",
      "Epoch [800/1000], Loss: 39.55182270310664\n",
      "Epoch [900/1000], Loss: 39.54004183089467\n",
      "Epoch [1000/1000], Loss: 39.54921430714927\n",
      "Epoch [100/1000], Loss: 39.47992799389134\n",
      "Epoch [200/1000], Loss: 39.48783784618732\n",
      "Epoch [300/1000], Loss: 39.48976503764236\n",
      "Epoch [400/1000], Loss: 39.47959092581137\n",
      "Epoch [500/1000], Loss: 39.47732561018707\n",
      "Epoch [600/1000], Loss: 39.47961101173936\n",
      "Epoch [700/1000], Loss: 39.49152492386845\n",
      "Epoch [800/1000], Loss: 39.486650246463\n",
      "Epoch [900/1000], Loss: 39.484085967505315\n",
      "Epoch [1000/1000], Loss: 39.48248620836089\n",
      "Epoch [100/1000], Loss: 39.21505610727716\n",
      "Epoch [200/1000], Loss: 39.21283888479281\n",
      "Epoch [300/1000], Loss: 39.20898005300245\n",
      "Epoch [400/1000], Loss: 39.207200831204794\n",
      "Epoch [500/1000], Loss: 39.20292501796898\n",
      "Epoch [600/1000], Loss: 39.19988503467715\n",
      "Epoch [700/1000], Loss: 39.20368387666546\n",
      "Epoch [800/1000], Loss: 39.21599395331771\n",
      "Epoch [900/1000], Loss: 39.2022349529251\n",
      "Epoch [1000/1000], Loss: 39.20075814483755\n",
      "Epoch [100/1000], Loss: 39.66010564782464\n",
      "Epoch [200/1000], Loss: 39.6599960296418\n",
      "Epoch [300/1000], Loss: 39.659949774517735\n",
      "Epoch [400/1000], Loss: 39.6512683032692\n",
      "Epoch [500/1000], Loss: 39.65272996869037\n",
      "Epoch [600/1000], Loss: 39.650497984370865\n",
      "Epoch [700/1000], Loss: 39.64819791743755\n",
      "Epoch [800/1000], Loss: 39.649207793421\n",
      "Epoch [900/1000], Loss: 39.65037218976865\n",
      "Epoch [1000/1000], Loss: 39.64852424506669\n",
      "Epoch [100/1000], Loss: 39.42757864324862\n",
      "Epoch [200/1000], Loss: 39.42932337606779\n",
      "Epoch [300/1000], Loss: 39.431097527537375\n",
      "Epoch [400/1000], Loss: 39.43376911107543\n",
      "Epoch [500/1000], Loss: 39.43092169857965\n",
      "Epoch [600/1000], Loss: 39.43132918710545\n",
      "Epoch [700/1000], Loss: 39.43874584236169\n",
      "Epoch [800/1000], Loss: 39.44116560503074\n",
      "Epoch [900/1000], Loss: 39.43442302884615\n",
      "Epoch [1000/1000], Loss: 39.4240317369646\n",
      "Epoch [100/1000], Loss: 39.34865758561888\n",
      "Epoch [200/1000], Loss: 39.3544114371273\n",
      "Epoch [300/1000], Loss: 39.34821246380524\n",
      "Epoch [400/1000], Loss: 39.34191075887763\n",
      "Epoch [500/1000], Loss: 39.34633596674255\n",
      "Epoch [600/1000], Loss: 39.351573164110604\n",
      "Epoch [700/1000], Loss: 39.33942566484837\n",
      "Epoch [800/1000], Loss: 39.34983450481448\n",
      "Epoch [900/1000], Loss: 39.357301153335705\n",
      "Epoch [1000/1000], Loss: 39.35039285766198\n",
      "Epoch [100/1000], Loss: 39.78087196447466\n",
      "Epoch [200/1000], Loss: 39.786970129628365\n",
      "Epoch [300/1000], Loss: 39.778958971101154\n",
      "Epoch [400/1000], Loss: 39.777500404398744\n",
      "Epoch [500/1000], Loss: 39.77502614888738\n",
      "Epoch [600/1000], Loss: 39.77593182926261\n",
      "Epoch [700/1000], Loss: 39.79078608957312\n",
      "Epoch [800/1000], Loss: 39.77914008417725\n",
      "Epoch [900/1000], Loss: 39.77899209740992\n",
      "Epoch [1000/1000], Loss: 39.789008858918024\n",
      "Epoch [100/1000], Loss: 39.96895038953285\n",
      "Epoch [200/1000], Loss: 39.97992615732051\n",
      "Epoch [300/1000], Loss: 39.98147274921028\n",
      "Epoch [400/1000], Loss: 39.99672448597067\n",
      "Epoch [500/1000], Loss: 39.98690096111102\n",
      "Epoch [600/1000], Loss: 39.9682821825211\n",
      "Epoch [700/1000], Loss: 39.988478859367625\n",
      "Epoch [800/1000], Loss: 40.002319761918365\n",
      "Epoch [900/1000], Loss: 39.970839949129704\n",
      "Epoch [1000/1000], Loss: 40.00409240107721\n",
      "Epoch [100/1000], Loss: 39.54392234702581\n",
      "Epoch [200/1000], Loss: 39.51463865292137\n",
      "Epoch [300/1000], Loss: 39.52549077080823\n",
      "Epoch [400/1000], Loss: 39.52292184530933\n",
      "Epoch [500/1000], Loss: 39.51728972975213\n",
      "Epoch [600/1000], Loss: 39.52638794928543\n",
      "Epoch [700/1000], Loss: 39.53335516777493\n",
      "Epoch [800/1000], Loss: 39.53613570491027\n",
      "Epoch [900/1000], Loss: 39.53107962561798\n",
      "Epoch [1000/1000], Loss: 39.51319147190655\n",
      "Epoch [100/1000], Loss: 39.8103365195313\n",
      "Epoch [200/1000], Loss: 39.79522517929401\n",
      "Epoch [300/1000], Loss: 39.79881688661101\n",
      "Epoch [400/1000], Loss: 39.810679819736364\n",
      "Epoch [500/1000], Loss: 39.80378006604835\n",
      "Epoch [600/1000], Loss: 39.79186383017546\n",
      "Epoch [700/1000], Loss: 39.8113228204799\n",
      "Epoch [800/1000], Loss: 39.80023167529391\n",
      "Epoch [900/1000], Loss: 39.79539545876214\n",
      "Epoch [1000/1000], Loss: 39.8094640247954\n",
      "Epoch [100/1000], Loss: 39.58250657138164\n",
      "Epoch [200/1000], Loss: 39.581722560793416\n",
      "Epoch [300/1000], Loss: 39.56225514168409\n",
      "Epoch [400/1000], Loss: 39.58208451250703\n",
      "Epoch [500/1000], Loss: 39.59040935321542\n",
      "Epoch [600/1000], Loss: 39.5825625969042\n",
      "Epoch [700/1000], Loss: 39.600724846019205\n",
      "Epoch [800/1000], Loss: 39.59566491350605\n",
      "Epoch [900/1000], Loss: 39.59524724926085\n",
      "Epoch [1000/1000], Loss: 39.57801097895897\n",
      "Epoch [100/1000], Loss: 39.52658970341376\n",
      "Epoch [200/1000], Loss: 39.52655788108055\n",
      "Epoch [300/1000], Loss: 39.508908089220675\n",
      "Epoch [400/1000], Loss: 39.51716086718589\n",
      "Epoch [500/1000], Loss: 39.52917706677416\n",
      "Epoch [600/1000], Loss: 39.51017575518228\n",
      "Epoch [700/1000], Loss: 39.51304233582488\n",
      "Epoch [800/1000], Loss: 39.51473865807585\n",
      "Epoch [900/1000], Loss: 39.5249658886343\n",
      "Epoch [1000/1000], Loss: 39.53244712105079\n",
      "Epoch [100/1000], Loss: 39.25239113760351\n",
      "Epoch [200/1000], Loss: 39.26217936156104\n",
      "Epoch [300/1000], Loss: 39.248634678964905\n",
      "Epoch [400/1000], Loss: 39.24578022730344\n",
      "Epoch [500/1000], Loss: 39.246686251335355\n",
      "Epoch [600/1000], Loss: 39.28548186437457\n",
      "Epoch [700/1000], Loss: 39.254336869978154\n",
      "Epoch [800/1000], Loss: 39.25574439533355\n",
      "Epoch [900/1000], Loss: 39.2506932740212\n",
      "Epoch [1000/1000], Loss: 39.27278836130086\n",
      "Epoch [100/1000], Loss: 39.70119545680144\n",
      "Epoch [200/1000], Loss: 39.702766939483375\n",
      "Epoch [300/1000], Loss: 39.70520508584235\n",
      "Epoch [400/1000], Loss: 39.70585160521541\n",
      "Epoch [500/1000], Loss: 39.702016310382646\n",
      "Epoch [600/1000], Loss: 39.70229493987126\n",
      "Epoch [700/1000], Loss: 39.687903614341245\n",
      "Epoch [800/1000], Loss: 39.70274764244256\n",
      "Epoch [900/1000], Loss: 39.70564707274371\n",
      "Epoch [1000/1000], Loss: 39.68202046374516\n",
      "Epoch [100/1000], Loss: 39.48951447248618\n",
      "Epoch [200/1000], Loss: 39.46619660265731\n",
      "Epoch [300/1000], Loss: 39.45948217622997\n",
      "Epoch [400/1000], Loss: 39.48167606291004\n",
      "Epoch [500/1000], Loss: 39.463182330168614\n",
      "Epoch [600/1000], Loss: 39.478252091086894\n",
      "Epoch [700/1000], Loss: 39.4678249582973\n",
      "Epoch [800/1000], Loss: 39.473583312363424\n",
      "Epoch [900/1000], Loss: 39.46805079825068\n",
      "Epoch [1000/1000], Loss: 39.462881922754036\n",
      "Epoch [100/1000], Loss: 39.39648837990752\n",
      "Epoch [200/1000], Loss: 39.41818175910541\n",
      "Epoch [300/1000], Loss: 39.39051701865267\n",
      "Epoch [400/1000], Loss: 39.39148863596406\n",
      "Epoch [500/1000], Loss: 39.381370834687374\n",
      "Epoch [600/1000], Loss: 39.3751326811358\n",
      "Epoch [700/1000], Loss: 39.409438456270315\n",
      "Epoch [800/1000], Loss: 39.38918881578358\n",
      "Epoch [900/1000], Loss: 39.39931450922544\n",
      "Epoch [1000/1000], Loss: 39.375766783521776\n",
      "Epoch [100/1000], Loss: 39.828629254774384\n",
      "Epoch [200/1000], Loss: 39.83637054077958\n",
      "Epoch [300/1000], Loss: 39.805384734722566\n",
      "Epoch [400/1000], Loss: 39.82309874949737\n",
      "Epoch [500/1000], Loss: 39.82790177543355\n",
      "Epoch [600/1000], Loss: 39.81043234753978\n",
      "Epoch [700/1000], Loss: 39.81313775984851\n",
      "Epoch [800/1000], Loss: 39.82828178318722\n",
      "Epoch [900/1000], Loss: 39.81153107172895\n",
      "Epoch [1000/1000], Loss: 39.82078403977109\n",
      "Epoch [100/1000], Loss: 40.04039963637007\n",
      "Epoch [200/1000], Loss: 40.02846461243192\n",
      "Epoch [300/1000], Loss: 40.0416089673302\n",
      "Epoch [400/1000], Loss: 40.063890824414266\n",
      "Epoch [500/1000], Loss: 40.05668814175866\n",
      "Epoch [600/1000], Loss: 40.031432156381\n",
      "Epoch [700/1000], Loss: 40.04799183982275\n",
      "Epoch [800/1000], Loss: 40.02533465702062\n",
      "Epoch [900/1000], Loss: 40.03621386742167\n",
      "Epoch [1000/1000], Loss: 40.019242411894695\n",
      "Epoch [100/1000], Loss: 39.60928318072667\n",
      "Epoch [200/1000], Loss: 39.55831043093259\n",
      "Epoch [300/1000], Loss: 39.569732836610335\n",
      "Epoch [400/1000], Loss: 39.5696217829785\n",
      "Epoch [500/1000], Loss: 39.53198729153621\n",
      "Epoch [600/1000], Loss: 39.58681032111868\n",
      "Epoch [700/1000], Loss: 39.57017799950056\n",
      "Epoch [800/1000], Loss: 39.582325798512166\n",
      "Epoch [900/1000], Loss: 39.57419990115674\n",
      "Epoch [1000/1000], Loss: 39.57932687549092\n",
      "Epoch [100/1000], Loss: 39.83523549161417\n",
      "Epoch [200/1000], Loss: 39.84157397178452\n",
      "Epoch [300/1000], Loss: 39.83923007365545\n",
      "Epoch [400/1000], Loss: 39.82669671085304\n",
      "Epoch [500/1000], Loss: 39.86026448177555\n",
      "Epoch [600/1000], Loss: 39.86080864117994\n",
      "Epoch [700/1000], Loss: 39.83884174147336\n",
      "Epoch [800/1000], Loss: 39.82661303051244\n",
      "Epoch [900/1000], Loss: 39.863382801468035\n",
      "Epoch [1000/1000], Loss: 39.84269728286244\n",
      "Epoch [100/1000], Loss: 39.63588877947346\n",
      "Epoch [200/1000], Loss: 39.64703354437843\n",
      "Epoch [300/1000], Loss: 39.629200609912516\n",
      "Epoch [400/1000], Loss: 39.65045064710315\n",
      "Epoch [500/1000], Loss: 39.65482860249445\n",
      "Epoch [600/1000], Loss: 39.63684069992844\n",
      "Epoch [700/1000], Loss: 39.64927214327949\n",
      "Epoch [800/1000], Loss: 39.64240381415746\n",
      "Epoch [900/1000], Loss: 39.626843000929085\n",
      "Epoch [1000/1000], Loss: 39.64742232189961\n",
      "Epoch [100/1000], Loss: 39.59709610705755\n",
      "Epoch [200/1000], Loss: 39.592766913863635\n",
      "Epoch [300/1000], Loss: 39.591290053646304\n",
      "Epoch [400/1000], Loss: 39.58152966458959\n",
      "Epoch [500/1000], Loss: 39.59765305314745\n",
      "Epoch [600/1000], Loss: 39.60146161229231\n",
      "Epoch [700/1000], Loss: 39.59631621975767\n",
      "Epoch [800/1000], Loss: 39.57334486359974\n",
      "Epoch [900/1000], Loss: 39.59259397377695\n",
      "Epoch [1000/1000], Loss: 39.58903803132257\n",
      "Epoch [100/1000], Loss: 39.34145964452112\n",
      "Epoch [200/1000], Loss: 39.32606376001173\n",
      "Epoch [300/1000], Loss: 39.2912853471413\n",
      "Epoch [400/1000], Loss: 39.32293519865556\n",
      "Epoch [500/1000], Loss: 39.316250021164656\n",
      "Epoch [600/1000], Loss: 39.31512473867352\n",
      "Epoch [700/1000], Loss: 39.31547275989794\n",
      "Epoch [800/1000], Loss: 39.31547947903164\n",
      "Epoch [900/1000], Loss: 39.30033828888288\n",
      "Epoch [1000/1000], Loss: 39.28529674118784\n",
      "Epoch [100/1000], Loss: 39.748204749303866\n",
      "Epoch [200/1000], Loss: 39.73498379581159\n",
      "Epoch [300/1000], Loss: 39.741233018728266\n",
      "Epoch [400/1000], Loss: 39.749317202429324\n",
      "Epoch [500/1000], Loss: 39.76666763983437\n",
      "Epoch [600/1000], Loss: 39.730143863369385\n",
      "Epoch [700/1000], Loss: 39.72815263049168\n",
      "Epoch [800/1000], Loss: 39.723612738449624\n",
      "Epoch [900/1000], Loss: 39.74980109810959\n",
      "Epoch [1000/1000], Loss: 39.75376772861195\n",
      "Epoch [100/1000], Loss: 39.54439357432881\n",
      "Epoch [200/1000], Loss: 39.53004585649621\n",
      "Epoch [300/1000], Loss: 39.50540800803495\n",
      "Epoch [400/1000], Loss: 39.535889603116566\n",
      "Epoch [500/1000], Loss: 39.51945328890258\n",
      "Epoch [600/1000], Loss: 39.53149860162068\n",
      "Epoch [700/1000], Loss: 39.54101891275968\n",
      "Epoch [800/1000], Loss: 39.53604338506381\n",
      "Epoch [900/1000], Loss: 39.53663792980069\n",
      "Epoch [1000/1000], Loss: 39.546239861784834\n",
      "Epoch [100/1000], Loss: 39.434143621950845\n",
      "Epoch [200/1000], Loss: 39.42763681889052\n",
      "Epoch [300/1000], Loss: 39.45903417507971\n",
      "Epoch [400/1000], Loss: 39.457502399263674\n",
      "Epoch [500/1000], Loss: 39.449483365075906\n",
      "Epoch [600/1000], Loss: 39.43605564734756\n",
      "Epoch [700/1000], Loss: 39.436516150268424\n",
      "Epoch [800/1000], Loss: 39.44774552235811\n",
      "Epoch [900/1000], Loss: 39.46545927630393\n",
      "Epoch [1000/1000], Loss: 39.452362600907904\n",
      "Epoch [100/1000], Loss: 39.903576046743424\n",
      "Epoch [200/1000], Loss: 39.88272823935926\n",
      "Epoch [300/1000], Loss: 39.85381732316297\n",
      "Epoch [400/1000], Loss: 39.884002249777836\n",
      "Epoch [500/1000], Loss: 39.85489006898979\n",
      "Epoch [600/1000], Loss: 39.86475060865162\n",
      "Epoch [700/1000], Loss: 39.8703099748912\n",
      "Epoch [800/1000], Loss: 39.89584161203021\n",
      "Epoch [900/1000], Loss: 39.858379856675334\n",
      "Epoch [1000/1000], Loss: 39.86032247529357\n",
      "Epoch [100/1000], Loss: 40.081903766002014\n",
      "Epoch [200/1000], Loss: 40.077561619528936\n",
      "Epoch [300/1000], Loss: 40.090126674464415\n",
      "Epoch [400/1000], Loss: 40.07604581575526\n",
      "Epoch [500/1000], Loss: 40.06562819978321\n",
      "Epoch [600/1000], Loss: 40.100277628468035\n",
      "Epoch [700/1000], Loss: 40.06091473916884\n",
      "Epoch [800/1000], Loss: 40.090687436515154\n",
      "Epoch [900/1000], Loss: 40.11799136305764\n",
      "Epoch [1000/1000], Loss: 40.08281400223512\n",
      "Epoch [100/1000], Loss: 39.63354162537398\n",
      "Epoch [200/1000], Loss: 39.61089011502843\n",
      "Epoch [300/1000], Loss: 39.6882115111224\n",
      "Epoch [400/1000], Loss: 39.61382905985347\n",
      "Epoch [500/1000], Loss: 39.62788033590207\n",
      "Epoch [600/1000], Loss: 39.58196444346108\n",
      "Epoch [700/1000], Loss: 39.630484793322296\n",
      "Epoch [800/1000], Loss: 39.63580892518805\n",
      "Epoch [900/1000], Loss: 39.66632487462903\n",
      "Epoch [1000/1000], Loss: 39.602063335657654\n",
      "Epoch [100/1000], Loss: 39.915322395063\n",
      "Epoch [200/1000], Loss: 39.931733832054626\n",
      "Epoch [300/1000], Loss: 39.9533295942336\n",
      "Epoch [400/1000], Loss: 39.892261240302226\n",
      "Epoch [500/1000], Loss: 39.883814047266796\n",
      "Epoch [600/1000], Loss: 39.90862177287763\n",
      "Epoch [700/1000], Loss: 39.91873609274501\n",
      "Epoch [800/1000], Loss: 39.93017537860681\n",
      "Epoch [900/1000], Loss: 39.893889078659655\n",
      "Epoch [1000/1000], Loss: 39.87864627161776\n",
      "Epoch [100/1000], Loss: 39.62805665595353\n",
      "Epoch [200/1000], Loss: 39.70174052143246\n",
      "Epoch [300/1000], Loss: 39.70730757660625\n",
      "Epoch [400/1000], Loss: 39.683082060660624\n",
      "Epoch [500/1000], Loss: 39.662656103724395\n",
      "Epoch [600/1000], Loss: 39.66268051944675\n",
      "Epoch [700/1000], Loss: 39.71064958853842\n",
      "Epoch [800/1000], Loss: 39.667079161900844\n",
      "Epoch [900/1000], Loss: 39.65426180747583\n",
      "Epoch [1000/1000], Loss: 39.687577674936975\n",
      "Epoch [100/1000], Loss: 39.649094338947535\n",
      "Epoch [200/1000], Loss: 39.592590502750824\n",
      "Epoch [300/1000], Loss: 39.5956410884161\n",
      "Epoch [400/1000], Loss: 39.63491194017085\n",
      "Epoch [500/1000], Loss: 39.63590341609144\n",
      "Epoch [600/1000], Loss: 39.6176323265836\n",
      "Epoch [700/1000], Loss: 39.62807925185097\n",
      "Epoch [800/1000], Loss: 39.60454081543298\n",
      "Epoch [900/1000], Loss: 39.609223372149145\n",
      "Epoch [1000/1000], Loss: 39.59629641272562\n",
      "Epoch [100/1000], Loss: 39.37772263917537\n",
      "Epoch [200/1000], Loss: 39.326239013909635\n",
      "Epoch [300/1000], Loss: 39.368649762291405\n",
      "Epoch [400/1000], Loss: 39.359091318288606\n",
      "Epoch [500/1000], Loss: 39.336540241082595\n",
      "Epoch [600/1000], Loss: 39.347496773914756\n",
      "Epoch [700/1000], Loss: 39.35353241638408\n",
      "Epoch [800/1000], Loss: 39.339361265361156\n",
      "Epoch [900/1000], Loss: 39.3926544674546\n",
      "Epoch [1000/1000], Loss: 39.31743648768608\n",
      "Epoch [100/1000], Loss: 39.84729338005438\n",
      "Epoch [200/1000], Loss: 39.80252312632442\n",
      "Epoch [300/1000], Loss: 39.77713005724797\n",
      "Epoch [400/1000], Loss: 39.785457161598124\n",
      "Epoch [500/1000], Loss: 39.80188464448133\n",
      "Epoch [600/1000], Loss: 39.77100760126161\n",
      "Epoch [700/1000], Loss: 39.8126267932937\n",
      "Epoch [800/1000], Loss: 39.77856126983603\n",
      "Epoch [900/1000], Loss: 39.77574261414604\n",
      "Epoch [1000/1000], Loss: 39.791383904289994\n",
      "Epoch [100/1000], Loss: 39.54683147628551\n",
      "Epoch [200/1000], Loss: 39.577489607228124\n",
      "Epoch [300/1000], Loss: 39.59367222957526\n",
      "Epoch [400/1000], Loss: 39.586746481829465\n",
      "Epoch [500/1000], Loss: 39.58068773349273\n",
      "Epoch [600/1000], Loss: 39.621033825991645\n",
      "Epoch [700/1000], Loss: 39.58241506997115\n",
      "Epoch [800/1000], Loss: 39.5814131803291\n",
      "Epoch [900/1000], Loss: 39.540376782293656\n",
      "Epoch [1000/1000], Loss: 39.55011074500225\n",
      "Epoch [100/1000], Loss: 39.498529114326345\n",
      "Epoch [200/1000], Loss: 39.51568908438809\n",
      "Epoch [300/1000], Loss: 39.472630253631706\n",
      "Epoch [400/1000], Loss: 39.4913764338669\n",
      "Epoch [500/1000], Loss: 39.49161354653474\n",
      "Epoch [600/1000], Loss: 39.49207533186156\n",
      "Epoch [700/1000], Loss: 39.478156048648195\n",
      "Epoch [800/1000], Loss: 39.46466676898141\n",
      "Epoch [900/1000], Loss: 39.49984541936031\n",
      "Epoch [1000/1000], Loss: 39.52781892125856\n",
      "Epoch [100/1000], Loss: 39.956445066203585\n",
      "Epoch [200/1000], Loss: 39.936438927270494\n",
      "Epoch [300/1000], Loss: 39.91933551147349\n",
      "Epoch [400/1000], Loss: 39.91548989660164\n",
      "Epoch [500/1000], Loss: 39.92521880641943\n",
      "Epoch [600/1000], Loss: 39.89713318670673\n",
      "Epoch [700/1000], Loss: 39.93752324683618\n",
      "Epoch [800/1000], Loss: 39.9348439796171\n",
      "Epoch [900/1000], Loss: 39.90339777134551\n",
      "Epoch [1000/1000], Loss: 39.93148475002731\n",
      "epoch: 100/1000, Loss: 519.5252242838718\n",
      "epoch: 200/1000, Loss: 292.4301810541025\n",
      "epoch: 300/1000, Loss: 238.04550695242247\n",
      "epoch: 400/1000, Loss: 219.71841257040953\n",
      "epoch: 500/1000, Loss: 209.47287022664236\n",
      "epoch: 600/1000, Loss: 201.51088430381986\n",
      "epoch: 700/1000, Loss: 194.5711389180004\n",
      "epoch: 800/1000, Loss: 188.32920894914824\n",
      "epoch: 900/1000, Loss: 182.66150419308696\n",
      "epoch: 1000/1000, Loss: 177.49353985530104\n",
      "epoch: 100/1000, Loss: 513.863245580336\n",
      "epoch: 200/1000, Loss: 288.2858252453403\n",
      "epoch: 300/1000, Loss: 234.82128383841768\n",
      "epoch: 400/1000, Loss: 217.14966074642393\n",
      "epoch: 500/1000, Loss: 207.42204165115848\n",
      "epoch: 600/1000, Loss: 199.8864022404318\n",
      "epoch: 700/1000, Loss: 193.30172856309815\n",
      "epoch: 800/1000, Loss: 187.35582570293417\n",
      "epoch: 900/1000, Loss: 181.93427078342117\n",
      "epoch: 1000/1000, Loss: 176.9701144239069\n",
      "epoch: 100/1000, Loss: 518.0940718689956\n",
      "epoch: 200/1000, Loss: 289.6516967512806\n",
      "epoch: 300/1000, Loss: 235.2789271795075\n",
      "epoch: 400/1000, Loss: 217.21188759081753\n",
      "epoch: 500/1000, Loss: 207.24238627061928\n",
      "epoch: 600/1000, Loss: 199.52720659981867\n",
      "epoch: 700/1000, Loss: 192.79924526796188\n",
      "epoch: 800/1000, Loss: 186.73633026429175\n",
      "epoch: 900/1000, Loss: 181.21849925030935\n",
      "epoch: 1000/1000, Loss: 176.17486769480678\n",
      "epoch: 100/1000, Loss: 519.5834392679358\n",
      "epoch: 200/1000, Loss: 293.4074869432988\n",
      "epoch: 300/1000, Loss: 239.42025275742188\n",
      "epoch: 400/1000, Loss: 221.41488444216142\n",
      "epoch: 500/1000, Loss: 211.4346253758824\n",
      "epoch: 600/1000, Loss: 203.67881651340744\n",
      "epoch: 700/1000, Loss: 196.89104577501521\n",
      "epoch: 800/1000, Loss: 190.7547251181252\n",
      "epoch: 900/1000, Loss: 185.15407745414117\n",
      "epoch: 1000/1000, Loss: 180.02153272684225\n",
      "epoch: 100/1000, Loss: 516.3528512078001\n",
      "epoch: 200/1000, Loss: 287.6259884502559\n",
      "epoch: 300/1000, Loss: 233.42192865291835\n",
      "epoch: 400/1000, Loss: 215.55901459841562\n",
      "epoch: 500/1000, Loss: 205.76891652547874\n",
      "epoch: 600/1000, Loss: 198.20464296471854\n",
      "epoch: 700/1000, Loss: 191.6027786806274\n",
      "epoch: 800/1000, Loss: 185.64502934677867\n",
      "epoch: 900/1000, Loss: 180.21467631750633\n",
      "epoch: 1000/1000, Loss: 175.24349559207008\n",
      "epoch: 100/1000, Loss: 509.85047384743086\n",
      "epoch: 200/1000, Loss: 287.930091937931\n",
      "epoch: 300/1000, Loss: 235.24492211215312\n",
      "epoch: 400/1000, Loss: 217.70575502215138\n",
      "epoch: 500/1000, Loss: 207.97564002349077\n",
      "epoch: 600/1000, Loss: 200.4160478421303\n",
      "epoch: 700/1000, Loss: 193.80904417868499\n",
      "epoch: 800/1000, Loss: 187.84566465876878\n",
      "epoch: 900/1000, Loss: 182.41073267448579\n",
      "epoch: 1000/1000, Loss: 177.4360191907327\n",
      "epoch: 100/1000, Loss: 523.9247365927682\n",
      "epoch: 200/1000, Loss: 295.84850333158744\n",
      "epoch: 300/1000, Loss: 241.42664686680595\n",
      "epoch: 400/1000, Loss: 223.14983974129967\n",
      "epoch: 500/1000, Loss: 212.93021108698585\n",
      "epoch: 600/1000, Loss: 204.96272675003405\n",
      "epoch: 700/1000, Loss: 197.99059800812893\n",
      "epoch: 800/1000, Loss: 191.69403883733563\n",
      "epoch: 900/1000, Loss: 185.95340811849957\n",
      "epoch: 1000/1000, Loss: 180.6976342750091\n",
      "epoch: 100/1000, Loss: 518.1581571431492\n",
      "epoch: 200/1000, Loss: 292.1171965464516\n",
      "epoch: 300/1000, Loss: 238.19177278612108\n",
      "epoch: 400/1000, Loss: 220.09856752037194\n",
      "epoch: 500/1000, Loss: 210.02397647383202\n",
      "epoch: 600/1000, Loss: 202.21757797375804\n",
      "epoch: 700/1000, Loss: 195.42839374710195\n",
      "epoch: 800/1000, Loss: 189.3326458473389\n",
      "epoch: 900/1000, Loss: 183.8055210507158\n",
      "epoch: 1000/1000, Loss: 178.77134730455842\n",
      "epoch: 100/1000, Loss: 518.070555728187\n",
      "epoch: 200/1000, Loss: 291.95249030623904\n",
      "epoch: 300/1000, Loss: 237.77258132387917\n",
      "epoch: 400/1000, Loss: 219.44360303789838\n",
      "epoch: 500/1000, Loss: 209.16079950437364\n",
      "epoch: 600/1000, Loss: 201.1673045159775\n",
      "epoch: 700/1000, Loss: 194.20759282992253\n",
      "epoch: 800/1000, Loss: 187.95504415417247\n",
      "epoch: 900/1000, Loss: 182.28285834741249\n",
      "epoch: 1000/1000, Loss: 177.11384623104942\n",
      "epoch: 100/1000, Loss: 519.2571934256431\n",
      "epoch: 200/1000, Loss: 289.65626807157986\n",
      "epoch: 300/1000, Loss: 235.05496944608447\n",
      "epoch: 400/1000, Loss: 216.87243386612292\n",
      "epoch: 500/1000, Loss: 206.8137570787391\n",
      "epoch: 600/1000, Loss: 199.03169276401093\n",
      "epoch: 700/1000, Loss: 192.25870389415456\n",
      "epoch: 800/1000, Loss: 186.17044546857156\n",
      "epoch: 900/1000, Loss: 180.64415281239675\n",
      "epoch: 1000/1000, Loss: 175.60617674524775\n",
      "epoch: 100/1000, Loss: 528.3956324760863\n",
      "epoch: 200/1000, Loss: 292.54664712143733\n",
      "epoch: 300/1000, Loss: 236.61638220974427\n",
      "epoch: 400/1000, Loss: 218.17384302482048\n",
      "epoch: 500/1000, Loss: 208.071991727787\n",
      "epoch: 600/1000, Loss: 200.279903057078\n",
      "epoch: 700/1000, Loss: 193.49350213817303\n",
      "epoch: 800/1000, Loss: 187.38348572304025\n",
      "epoch: 900/1000, Loss: 181.82838808118856\n",
      "epoch: 1000/1000, Loss: 176.75669575104538\n",
      "epoch: 100/1000, Loss: 514.4748305643615\n",
      "epoch: 200/1000, Loss: 290.4714055631666\n",
      "epoch: 300/1000, Loss: 237.15372549130734\n",
      "epoch: 400/1000, Loss: 219.35662064786538\n",
      "epoch: 500/1000, Loss: 209.46393017548223\n",
      "epoch: 600/1000, Loss: 201.7690448753369\n",
      "epoch: 700/1000, Loss: 195.03852550060037\n",
      "epoch: 800/1000, Loss: 188.96029291951737\n",
      "epoch: 900/1000, Loss: 183.4187240480339\n",
      "epoch: 1000/1000, Loss: 178.34556989975553\n",
      "epoch: 100/1000, Loss: 522.144149109637\n",
      "epoch: 200/1000, Loss: 290.42542697054085\n",
      "epoch: 300/1000, Loss: 235.40156260994036\n",
      "epoch: 400/1000, Loss: 217.2131367822014\n",
      "epoch: 500/1000, Loss: 207.2257919933088\n",
      "epoch: 600/1000, Loss: 199.51040908633792\n",
      "epoch: 700/1000, Loss: 192.7831780358381\n",
      "epoch: 800/1000, Loss: 186.7191653199718\n",
      "epoch: 900/1000, Loss: 181.19835659029212\n",
      "epoch: 1000/1000, Loss: 176.15026047802039\n",
      "epoch: 100/1000, Loss: 517.4258632030164\n",
      "epoch: 200/1000, Loss: 293.214385562864\n",
      "epoch: 300/1000, Loss: 239.5519550112986\n",
      "epoch: 400/1000, Loss: 221.54629271626507\n",
      "epoch: 500/1000, Loss: 211.50810940341213\n",
      "epoch: 600/1000, Loss: 203.69026301277293\n",
      "epoch: 700/1000, Loss: 196.84627850004648\n",
      "epoch: 800/1000, Loss: 190.66074056760107\n",
      "epoch: 900/1000, Loss: 185.01737647675682\n",
      "epoch: 1000/1000, Loss: 179.84788775421356\n",
      "epoch: 100/1000, Loss: 511.0189670632789\n",
      "epoch: 200/1000, Loss: 287.1952584626443\n",
      "epoch: 300/1000, Loss: 233.92183456415262\n",
      "epoch: 400/1000, Loss: 216.18746435229633\n",
      "epoch: 500/1000, Loss: 206.36912467896025\n",
      "epoch: 600/1000, Loss: 198.7505353104694\n",
      "epoch: 700/1000, Loss: 192.09421222203235\n",
      "epoch: 800/1000, Loss: 186.0865327672581\n",
      "epoch: 900/1000, Loss: 180.6112001711966\n",
      "epoch: 1000/1000, Loss: 175.59961691388526\n",
      "epoch: 100/1000, Loss: 511.813255840097\n",
      "epoch: 200/1000, Loss: 289.16380381930895\n",
      "epoch: 300/1000, Loss: 236.27703895554237\n",
      "epoch: 400/1000, Loss: 218.65064593154446\n",
      "epoch: 500/1000, Loss: 208.8623975845162\n",
      "epoch: 600/1000, Loss: 201.25535184162436\n",
      "epoch: 700/1000, Loss: 194.6069645835043\n",
      "epoch: 800/1000, Loss: 188.60664205050804\n",
      "epoch: 900/1000, Loss: 183.13832424727408\n",
      "epoch: 1000/1000, Loss: 178.1331620245596\n",
      "epoch: 100/1000, Loss: 516.790246082384\n",
      "epoch: 200/1000, Loss: 293.9910398142452\n",
      "epoch: 300/1000, Loss: 240.6164558454541\n",
      "epoch: 400/1000, Loss: 222.5421575459333\n",
      "epoch: 500/1000, Loss: 212.36489733981298\n",
      "epoch: 600/1000, Loss: 204.4157189913837\n",
      "epoch: 700/1000, Loss: 197.46302988774303\n",
      "epoch: 800/1000, Loss: 191.1902571673107\n",
      "epoch: 900/1000, Loss: 185.47695371015644\n",
      "epoch: 1000/1000, Loss: 180.25077888464287\n",
      "epoch: 100/1000, Loss: 520.5633724998409\n",
      "epoch: 200/1000, Loss: 293.95797991350173\n",
      "epoch: 300/1000, Loss: 239.80827284223884\n",
      "epoch: 400/1000, Loss: 221.56945649552026\n",
      "epoch: 500/1000, Loss: 211.37376634023158\n",
      "epoch: 600/1000, Loss: 203.4588892417355\n",
      "epoch: 700/1000, Loss: 196.57066011071026\n",
      "epoch: 800/1000, Loss: 190.38397977240155\n",
      "epoch: 900/1000, Loss: 184.77317478127588\n",
      "epoch: 1000/1000, Loss: 179.66187629368966\n",
      "epoch: 100/1000, Loss: 518.0985841239686\n",
      "epoch: 200/1000, Loss: 291.81326852474774\n",
      "epoch: 300/1000, Loss: 237.7164993617697\n",
      "epoch: 400/1000, Loss: 219.50283295968111\n",
      "epoch: 500/1000, Loss: 209.3261266057748\n",
      "epoch: 600/1000, Loss: 201.42358131359018\n",
      "epoch: 700/1000, Loss: 194.54052823032654\n",
      "epoch: 800/1000, Loss: 188.35223866536705\n",
      "epoch: 900/1000, Loss: 182.73373128672924\n",
      "epoch: 1000/1000, Loss: 177.60938814098918\n",
      "epoch: 100/1000, Loss: 523.5108104015115\n",
      "epoch: 200/1000, Loss: 289.50491209540246\n",
      "epoch: 300/1000, Loss: 234.12527978397213\n",
      "epoch: 400/1000, Loss: 215.88613743285077\n",
      "epoch: 500/1000, Loss: 205.90422525815396\n",
      "epoch: 600/1000, Loss: 198.21466118191\n",
      "epoch: 700/1000, Loss: 191.52766016702637\n",
      "epoch: 800/1000, Loss: 185.51574613653787\n",
      "epoch: 900/1000, Loss: 180.05668079546095\n",
      "epoch: 1000/1000, Loss: 175.07789702010368\n",
      "epoch: 100/1000, Loss: 524.3233610789019\n",
      "epoch: 200/1000, Loss: 293.17316436111696\n",
      "epoch: 300/1000, Loss: 238.09582841627864\n",
      "epoch: 400/1000, Loss: 219.73806883327953\n",
      "epoch: 500/1000, Loss: 209.57699536680497\n",
      "epoch: 600/1000, Loss: 201.70691454942968\n",
      "epoch: 700/1000, Loss: 194.8476071641177\n",
      "epoch: 800/1000, Loss: 188.67422593775612\n",
      "epoch: 900/1000, Loss: 183.06390339509713\n",
      "epoch: 1000/1000, Loss: 177.94405960934148\n",
      "epoch: 100/1000, Loss: 516.0521416825467\n",
      "epoch: 200/1000, Loss: 290.5582328866935\n",
      "epoch: 300/1000, Loss: 237.00009769948576\n",
      "epoch: 400/1000, Loss: 219.21415053813504\n",
      "epoch: 500/1000, Loss: 209.38047014533743\n",
      "epoch: 600/1000, Loss: 201.75069291881283\n",
      "epoch: 700/1000, Loss: 195.08282530279925\n",
      "epoch: 800/1000, Loss: 189.06300922101985\n",
      "epoch: 900/1000, Loss: 183.575315173745\n",
      "epoch: 1000/1000, Loss: 178.5515471113224\n",
      "epoch: 100/1000, Loss: 518.7746534868841\n",
      "epoch: 200/1000, Loss: 289.4676818294588\n",
      "epoch: 300/1000, Loss: 235.00681636288363\n",
      "epoch: 400/1000, Loss: 216.99737399580292\n",
      "epoch: 500/1000, Loss: 207.1046217075748\n",
      "epoch: 600/1000, Loss: 199.4612702001145\n",
      "epoch: 700/1000, Loss: 192.79672784787502\n",
      "epoch: 800/1000, Loss: 186.78927708954373\n",
      "epoch: 900/1000, Loss: 181.3200074893483\n",
      "epoch: 1000/1000, Loss: 176.3190409752885\n",
      "epoch: 100/1000, Loss: 518.2718821489083\n",
      "epoch: 200/1000, Loss: 293.5372269322279\n",
      "epoch: 300/1000, Loss: 239.79021111846615\n",
      "epoch: 400/1000, Loss: 221.78966482941362\n",
      "epoch: 500/1000, Loss: 211.77299136487443\n",
      "epoch: 600/1000, Loss: 203.97828021227107\n",
      "epoch: 700/1000, Loss: 197.15614372796105\n",
      "epoch: 800/1000, Loss: 190.99068077700846\n",
      "epoch: 900/1000, Loss: 185.36562837847973\n",
      "epoch: 1000/1000, Loss: 180.21279181730247\n",
      "epoch: 100/1000, Loss: 515.4440193689077\n",
      "epoch: 200/1000, Loss: 286.684032509856\n",
      "epoch: 300/1000, Loss: 232.5162036758581\n",
      "epoch: 400/1000, Loss: 214.70398697150407\n",
      "epoch: 500/1000, Loss: 204.96728781159746\n",
      "epoch: 600/1000, Loss: 197.45668852499347\n",
      "epoch: 700/1000, Loss: 190.90806804186798\n",
      "epoch: 800/1000, Loss: 185.00212493591192\n",
      "epoch: 900/1000, Loss: 179.62189827305184\n",
      "epoch: 1000/1000, Loss: 174.69901194533406\n",
      "epoch: 100/1000, Loss: 507.58507528506146\n",
      "epoch: 200/1000, Loss: 288.68156961806835\n",
      "epoch: 300/1000, Loss: 236.48919717695037\n",
      "epoch: 400/1000, Loss: 218.94707878557878\n",
      "epoch: 500/1000, Loss: 209.12672648675914\n",
      "epoch: 600/1000, Loss: 201.47071994983227\n",
      "epoch: 700/1000, Loss: 194.7760302884364\n",
      "epoch: 800/1000, Loss: 188.73538790054047\n",
      "epoch: 900/1000, Loss: 183.23349387069885\n",
      "epoch: 1000/1000, Loss: 178.2006470993554\n",
      "epoch: 100/1000, Loss: 522.0354985402099\n",
      "epoch: 200/1000, Loss: 294.65415746767496\n",
      "epoch: 300/1000, Loss: 240.37990354156582\n",
      "epoch: 400/1000, Loss: 222.15581435234006\n",
      "epoch: 500/1000, Loss: 211.98141175936996\n",
      "epoch: 600/1000, Loss: 204.0658609577167\n",
      "epoch: 700/1000, Loss: 197.1525044879135\n",
      "epoch: 800/1000, Loss: 190.91947313834822\n",
      "epoch: 900/1000, Loss: 185.2450999690123\n",
      "epoch: 1000/1000, Loss: 180.05669653155798\n",
      "epoch: 100/1000, Loss: 521.9607635993568\n",
      "epoch: 200/1000, Loss: 294.7830100824411\n",
      "epoch: 300/1000, Loss: 240.45348890936896\n",
      "epoch: 400/1000, Loss: 222.13682548160213\n",
      "epoch: 500/1000, Loss: 211.89940153814416\n",
      "epoch: 600/1000, Loss: 203.9617021177661\n",
      "epoch: 700/1000, Loss: 197.06309592906112\n",
      "epoch: 800/1000, Loss: 190.87486662819376\n",
      "epoch: 900/1000, Loss: 185.26918470841838\n",
      "epoch: 1000/1000, Loss: 180.16808723587167\n",
      "epoch: 100/1000, Loss: 514.5820777179506\n",
      "epoch: 200/1000, Loss: 290.68755522598025\n",
      "epoch: 300/1000, Loss: 237.17144362787562\n",
      "epoch: 400/1000, Loss: 219.15199224519765\n",
      "epoch: 500/1000, Loss: 209.07647119758664\n",
      "epoch: 600/1000, Loss: 201.2447613906274\n",
      "epoch: 700/1000, Loss: 194.41731426816582\n",
      "epoch: 800/1000, Loss: 188.27435100496322\n",
      "epoch: 900/1000, Loss: 182.69337894841127\n",
      "epoch: 1000/1000, Loss: 177.60045764404435\n",
      "epoch: 100/1000, Loss: 516.1472452168225\n",
      "epoch: 200/1000, Loss: 289.0078163621918\n",
      "epoch: 300/1000, Loss: 234.9149012865922\n",
      "epoch: 400/1000, Loss: 216.85193522449083\n",
      "epoch: 500/1000, Loss: 206.83878010031154\n",
      "epoch: 600/1000, Loss: 199.0903769144019\n",
      "epoch: 700/1000, Loss: 192.35044751131295\n",
      "epoch: 800/1000, Loss: 186.29590207276323\n",
      "epoch: 900/1000, Loss: 180.80347193735562\n",
      "epoch: 1000/1000, Loss: 175.79885933472517\n",
      "epoch: 100/1000, Loss: 578.8074652604204\n",
      "epoch: 200/1000, Loss: 348.1121877228636\n",
      "epoch: 300/1000, Loss: 292.56551788530305\n",
      "epoch: 400/1000, Loss: 274.27043852293565\n",
      "epoch: 500/1000, Loss: 264.3302274954213\n",
      "epoch: 600/1000, Loss: 256.65004899730894\n",
      "epoch: 700/1000, Loss: 249.96142203382772\n",
      "epoch: 800/1000, Loss: 243.90981273785513\n",
      "epoch: 900/1000, Loss: 238.4729674046364\n",
      "epoch: 1000/1000, Loss: 233.56919657252246\n",
      "epoch: 100/1000, Loss: 571.7209700146171\n",
      "epoch: 200/1000, Loss: 347.5603573869451\n",
      "epoch: 300/1000, Loss: 293.64642062720975\n",
      "epoch: 400/1000, Loss: 275.9375023122724\n",
      "epoch: 500/1000, Loss: 266.294624101362\n",
      "epoch: 600/1000, Loss: 258.8032370662206\n",
      "epoch: 700/1000, Loss: 252.24450631340295\n",
      "epoch: 800/1000, Loss: 246.31286918485682\n",
      "epoch: 900/1000, Loss: 240.86474399409036\n",
      "epoch: 1000/1000, Loss: 235.8786858472373\n",
      "epoch: 100/1000, Loss: 568.3894912676087\n",
      "epoch: 200/1000, Loss: 345.5192675508207\n",
      "epoch: 300/1000, Loss: 291.73693439476693\n",
      "epoch: 400/1000, Loss: 273.97737519729503\n",
      "epoch: 500/1000, Loss: 264.3088111863647\n",
      "epoch: 600/1000, Loss: 256.82025872085035\n",
      "epoch: 700/1000, Loss: 250.29785801997605\n",
      "epoch: 800/1000, Loss: 244.42941284211093\n",
      "epoch: 900/1000, Loss: 239.10531562830766\n",
      "epoch: 1000/1000, Loss: 234.33437588792236\n",
      "epoch: 100/1000, Loss: 577.5597252310686\n",
      "epoch: 200/1000, Loss: 351.65093961082874\n",
      "epoch: 300/1000, Loss: 297.100647751956\n",
      "epoch: 400/1000, Loss: 279.10059475695664\n",
      "epoch: 500/1000, Loss: 269.30791742106936\n",
      "epoch: 600/1000, Loss: 261.74506228031146\n",
      "epoch: 700/1000, Loss: 255.07262039912484\n",
      "epoch: 800/1000, Loss: 249.02040591189737\n",
      "epoch: 900/1000, Loss: 243.4833234955522\n",
      "epoch: 1000/1000, Loss: 238.39837848548163\n",
      "epoch: 100/1000, Loss: 569.2314656247972\n",
      "epoch: 200/1000, Loss: 343.5271909737688\n",
      "epoch: 300/1000, Loss: 289.50882265930534\n",
      "epoch: 400/1000, Loss: 271.90313101187644\n",
      "epoch: 500/1000, Loss: 262.40618217659784\n",
      "epoch: 600/1000, Loss: 255.1666193278736\n",
      "epoch: 700/1000, Loss: 248.83079175687456\n",
      "epoch: 800/1000, Loss: 243.0921905766433\n",
      "epoch: 900/1000, Loss: 237.8571517201429\n",
      "epoch: 1000/1000, Loss: 233.06428918546246\n",
      "epoch: 100/1000, Loss: 566.6634232937048\n",
      "epoch: 200/1000, Loss: 345.88592305310453\n",
      "epoch: 300/1000, Loss: 292.858054422571\n",
      "epoch: 400/1000, Loss: 275.4384882468644\n",
      "epoch: 500/1000, Loss: 265.9224614451754\n",
      "epoch: 600/1000, Loss: 258.5340463977479\n",
      "epoch: 700/1000, Loss: 252.09473535677827\n",
      "epoch: 800/1000, Loss: 246.25954012982058\n",
      "epoch: 900/1000, Loss: 240.92743469274998\n",
      "epoch: 1000/1000, Loss: 236.03778224702614\n",
      "epoch: 100/1000, Loss: 580.497507455244\n",
      "epoch: 200/1000, Loss: 353.7083651674941\n",
      "epoch: 300/1000, Loss: 298.8997243063032\n",
      "epoch: 400/1000, Loss: 280.56399559515614\n",
      "epoch: 500/1000, Loss: 270.42168752744954\n",
      "epoch: 600/1000, Loss: 262.5312475657032\n",
      "epoch: 700/1000, Loss: 255.62726987866972\n",
      "epoch: 800/1000, Loss: 249.38781261449134\n",
      "epoch: 900/1000, Loss: 243.67514465524664\n",
      "epoch: 1000/1000, Loss: 238.436176060734\n",
      "epoch: 100/1000, Loss: 568.4601200604885\n",
      "epoch: 200/1000, Loss: 348.1882320978761\n",
      "epoch: 300/1000, Loss: 294.67738030993183\n",
      "epoch: 400/1000, Loss: 276.7199248961065\n",
      "epoch: 500/1000, Loss: 266.7771736501379\n",
      "epoch: 600/1000, Loss: 259.1029377727965\n",
      "epoch: 700/1000, Loss: 252.41780109972075\n",
      "epoch: 800/1000, Loss: 246.3896363696457\n",
      "epoch: 900/1000, Loss: 240.92443010379066\n",
      "epoch: 1000/1000, Loss: 235.9756021683375\n",
      "epoch: 100/1000, Loss: 565.3673131799411\n",
      "epoch: 200/1000, Loss: 347.12606287779454\n",
      "epoch: 300/1000, Loss: 293.91872329379606\n",
      "epoch: 400/1000, Loss: 276.00587783503386\n",
      "epoch: 500/1000, Loss: 266.08384667901925\n",
      "epoch: 600/1000, Loss: 258.333361304319\n",
      "epoch: 700/1000, Loss: 251.54517356517493\n",
      "epoch: 800/1000, Loss: 245.44220079424537\n",
      "epoch: 900/1000, Loss: 239.8861952594791\n",
      "epoch: 1000/1000, Loss: 234.88513536918288\n",
      "epoch: 100/1000, Loss: 571.406237456194\n",
      "epoch: 200/1000, Loss: 345.8013205774758\n",
      "epoch: 300/1000, Loss: 291.4342271374118\n",
      "epoch: 400/1000, Loss: 273.43070740458825\n",
      "epoch: 500/1000, Loss: 263.5926204768479\n",
      "epoch: 600/1000, Loss: 256.00158643595535\n",
      "epoch: 700/1000, Loss: 249.3374108047853\n",
      "epoch: 800/1000, Loss: 243.34085575139818\n",
      "epoch: 900/1000, Loss: 237.90945219305578\n",
      "epoch: 1000/1000, Loss: 232.9657302231126\n",
      "epoch: 100/1000, Loss: 649.8843351016523\n",
      "epoch: 200/1000, Loss: 438.9162352963167\n",
      "epoch: 300/1000, Loss: 385.11210709528154\n",
      "epoch: 400/1000, Loss: 366.2078095217537\n",
      "epoch: 500/1000, Loss: 355.6798270146645\n",
      "epoch: 600/1000, Loss: 347.50715984855293\n",
      "epoch: 700/1000, Loss: 340.3066572791399\n",
      "epoch: 800/1000, Loss: 333.5086783560163\n",
      "epoch: 900/1000, Loss: 327.2777409923441\n",
      "epoch: 1000/1000, Loss: 321.4131544165766\n",
      "epoch: 100/1000, Loss: 641.0077870249173\n",
      "epoch: 200/1000, Loss: 436.184421259744\n",
      "epoch: 300/1000, Loss: 384.526557754479\n",
      "epoch: 400/1000, Loss: 366.8477694646307\n",
      "epoch: 500/1000, Loss: 357.0589906497014\n",
      "epoch: 600/1000, Loss: 349.44352223745796\n",
      "epoch: 700/1000, Loss: 342.68913644998213\n",
      "epoch: 800/1000, Loss: 336.4698301440051\n",
      "epoch: 900/1000, Loss: 330.68416536657674\n",
      "epoch: 1000/1000, Loss: 325.0602980343293\n",
      "epoch: 100/1000, Loss: 638.4521559442248\n",
      "epoch: 200/1000, Loss: 435.38494670185605\n",
      "epoch: 300/1000, Loss: 384.02863492933534\n",
      "epoch: 400/1000, Loss: 366.34239456497676\n",
      "epoch: 500/1000, Loss: 356.6553722377809\n",
      "epoch: 600/1000, Loss: 349.1461405855526\n",
      "epoch: 700/1000, Loss: 342.44136414832303\n",
      "epoch: 800/1000, Loss: 336.31312313979663\n",
      "epoch: 900/1000, Loss: 330.48439786572305\n",
      "epoch: 1000/1000, Loss: 324.94118161909427\n",
      "epoch: 100/1000, Loss: 645.4215455460787\n",
      "epoch: 200/1000, Loss: 439.705781998584\n",
      "epoch: 300/1000, Loss: 387.50130703715024\n",
      "epoch: 400/1000, Loss: 369.5584056988513\n",
      "epoch: 500/1000, Loss: 359.5302605676225\n",
      "epoch: 600/1000, Loss: 351.681278465196\n",
      "epoch: 700/1000, Loss: 344.59834757016574\n",
      "epoch: 800/1000, Loss: 338.05578147128693\n",
      "epoch: 900/1000, Loss: 331.8521254464587\n",
      "epoch: 1000/1000, Loss: 325.96855543403353\n",
      "epoch: 100/1000, Loss: 634.8044215653402\n",
      "epoch: 200/1000, Loss: 431.22087651627317\n",
      "epoch: 300/1000, Loss: 379.8103522875657\n",
      "epoch: 400/1000, Loss: 362.2383527577257\n",
      "epoch: 500/1000, Loss: 352.5008404297327\n",
      "epoch: 600/1000, Loss: 344.90584224111717\n",
      "epoch: 700/1000, Loss: 338.1778367471779\n",
      "epoch: 800/1000, Loss: 331.92836402339873\n",
      "epoch: 900/1000, Loss: 325.9289582393793\n",
      "epoch: 1000/1000, Loss: 320.287063169358\n",
      "epoch: 100/1000, Loss: 631.2623753046273\n",
      "epoch: 200/1000, Loss: 432.45927515300724\n",
      "epoch: 300/1000, Loss: 382.05308976531023\n",
      "epoch: 400/1000, Loss: 364.57925845602654\n",
      "epoch: 500/1000, Loss: 354.76600912273676\n",
      "epoch: 600/1000, Loss: 347.07045173658184\n",
      "epoch: 700/1000, Loss: 340.1910014340121\n",
      "epoch: 800/1000, Loss: 333.8686077905435\n",
      "epoch: 900/1000, Loss: 327.8072658005922\n",
      "epoch: 1000/1000, Loss: 322.08307302261295\n",
      "epoch: 100/1000, Loss: 649.6810447396176\n",
      "epoch: 200/1000, Loss: 442.09181848475475\n",
      "epoch: 300/1000, Loss: 389.51404728704097\n",
      "epoch: 400/1000, Loss: 371.3994687066676\n",
      "epoch: 500/1000, Loss: 361.24425688172175\n",
      "epoch: 600/1000, Loss: 353.30375110130086\n",
      "epoch: 700/1000, Loss: 346.4134233969237\n",
      "epoch: 800/1000, Loss: 339.8773025124865\n",
      "epoch: 900/1000, Loss: 333.64572141426345\n",
      "epoch: 1000/1000, Loss: 327.88334984616483\n",
      "epoch: 100/1000, Loss: 643.8229300365025\n",
      "epoch: 200/1000, Loss: 438.2124276160801\n",
      "epoch: 300/1000, Loss: 386.0716462365087\n",
      "epoch: 400/1000, Loss: 367.93448623158497\n",
      "epoch: 500/1000, Loss: 357.66184854217397\n",
      "epoch: 600/1000, Loss: 349.6124909925628\n",
      "epoch: 700/1000, Loss: 342.5588180639494\n",
      "epoch: 800/1000, Loss: 335.9822392788442\n",
      "epoch: 900/1000, Loss: 329.757838165545\n",
      "epoch: 1000/1000, Loss: 323.9871784727311\n",
      "epoch: 100/1000, Loss: 637.6911997895726\n",
      "epoch: 200/1000, Loss: 435.7159678371935\n",
      "epoch: 300/1000, Loss: 384.5629788088753\n",
      "epoch: 400/1000, Loss: 366.90806405248077\n",
      "epoch: 500/1000, Loss: 356.99476077861686\n",
      "epoch: 600/1000, Loss: 349.2448386519565\n",
      "epoch: 700/1000, Loss: 342.48470745757766\n",
      "epoch: 800/1000, Loss: 336.04195059405754\n",
      "epoch: 900/1000, Loss: 329.91888439751335\n",
      "epoch: 1000/1000, Loss: 323.9716139006597\n",
      "epoch: 100/1000, Loss: 641.6574856614246\n",
      "epoch: 200/1000, Loss: 434.8367626063\n",
      "epoch: 300/1000, Loss: 382.58301567487945\n",
      "epoch: 400/1000, Loss: 364.5810661248428\n",
      "epoch: 500/1000, Loss: 354.6899456854014\n",
      "epoch: 600/1000, Loss: 346.9617413574588\n",
      "epoch: 700/1000, Loss: 340.07483605131654\n",
      "epoch: 800/1000, Loss: 333.7168971253158\n",
      "epoch: 900/1000, Loss: 327.83372772956574\n",
      "epoch: 1000/1000, Loss: 322.15048601768166\n",
      "epoch: 100/1000, Loss: 709.1418584712029\n",
      "epoch: 200/1000, Loss: 512.8969494239949\n",
      "epoch: 300/1000, Loss: 461.9660288496339\n",
      "epoch: 400/1000, Loss: 441.90093901200015\n",
      "epoch: 500/1000, Loss: 428.9209362040101\n",
      "epoch: 600/1000, Loss: 418.32591498130284\n",
      "epoch: 700/1000, Loss: 408.5311011222882\n",
      "epoch: 800/1000, Loss: 399.40212817290217\n",
      "epoch: 900/1000, Loss: 390.4634078713352\n",
      "epoch: 1000/1000, Loss: 381.9490172597789\n",
      "epoch: 100/1000, Loss: 695.8251342953729\n",
      "epoch: 200/1000, Loss: 504.60577077391605\n",
      "epoch: 300/1000, Loss: 455.46064687486546\n",
      "epoch: 400/1000, Loss: 436.38819125617283\n",
      "epoch: 500/1000, Loss: 424.34580450532815\n",
      "epoch: 600/1000, Loss: 414.44152208950175\n",
      "epoch: 700/1000, Loss: 405.33794920168395\n",
      "epoch: 800/1000, Loss: 396.92104523922865\n",
      "epoch: 900/1000, Loss: 388.68148641668887\n",
      "epoch: 1000/1000, Loss: 380.75291003449956\n",
      "epoch: 100/1000, Loss: 703.3541996608858\n",
      "epoch: 200/1000, Loss: 509.05614320981715\n",
      "epoch: 300/1000, Loss: 459.06582945773175\n",
      "epoch: 400/1000, Loss: 439.72516600459966\n",
      "epoch: 500/1000, Loss: 427.5951577877073\n",
      "epoch: 600/1000, Loss: 417.5445533466328\n",
      "epoch: 700/1000, Loss: 408.1856856350799\n",
      "epoch: 800/1000, Loss: 399.48716039609405\n",
      "epoch: 900/1000, Loss: 391.0239956950563\n",
      "epoch: 1000/1000, Loss: 382.84050523491067\n",
      "epoch: 100/1000, Loss: 702.0534813563129\n",
      "epoch: 200/1000, Loss: 510.04108983184597\n",
      "epoch: 300/1000, Loss: 460.3836308446324\n",
      "epoch: 400/1000, Loss: 441.10784129506123\n",
      "epoch: 500/1000, Loss: 428.9453609768042\n",
      "epoch: 600/1000, Loss: 418.89825210040954\n",
      "epoch: 700/1000, Loss: 409.7337623923386\n",
      "epoch: 800/1000, Loss: 401.1213024064279\n",
      "epoch: 900/1000, Loss: 392.6572357392952\n",
      "epoch: 1000/1000, Loss: 384.47122130688126\n",
      "epoch: 100/1000, Loss: 688.6208422956465\n",
      "epoch: 200/1000, Loss: 500.48381296758146\n",
      "epoch: 300/1000, Loss: 451.8617774848112\n",
      "epoch: 400/1000, Loss: 432.8469163228717\n",
      "epoch: 500/1000, Loss: 420.53760164978485\n",
      "epoch: 600/1000, Loss: 410.43663179947794\n",
      "epoch: 700/1000, Loss: 401.08260894179887\n",
      "epoch: 800/1000, Loss: 392.45530212366396\n",
      "epoch: 900/1000, Loss: 384.19116968518586\n",
      "epoch: 1000/1000, Loss: 376.2869838441767\n",
      "epoch: 100/1000, Loss: 683.9559199197604\n",
      "epoch: 200/1000, Loss: 500.1658596611666\n",
      "epoch: 300/1000, Loss: 452.687653857068\n",
      "epoch: 400/1000, Loss: 433.9779507411461\n",
      "epoch: 500/1000, Loss: 421.80280681390866\n",
      "epoch: 600/1000, Loss: 411.7117357803429\n",
      "epoch: 700/1000, Loss: 402.4894917059431\n",
      "epoch: 800/1000, Loss: 394.0409753878138\n",
      "epoch: 900/1000, Loss: 385.76573421526575\n",
      "epoch: 1000/1000, Loss: 377.8705293817658\n",
      "epoch: 100/1000, Loss: 701.531862663814\n",
      "epoch: 200/1000, Loss: 512.6410116510372\n",
      "epoch: 300/1000, Loss: 463.70450549407417\n",
      "epoch: 400/1000, Loss: 444.5492690410971\n",
      "epoch: 500/1000, Loss: 432.5403206737964\n",
      "epoch: 600/1000, Loss: 422.5061761024763\n",
      "epoch: 700/1000, Loss: 413.2464300380881\n",
      "epoch: 800/1000, Loss: 404.5096396066477\n",
      "epoch: 900/1000, Loss: 396.1199627142174\n",
      "epoch: 1000/1000, Loss: 388.17311862030715\n",
      "epoch: 100/1000, Loss: 698.5918370667384\n",
      "epoch: 200/1000, Loss: 507.67722686162006\n",
      "epoch: 300/1000, Loss: 458.1875950703811\n",
      "epoch: 400/1000, Loss: 438.674857806526\n",
      "epoch: 500/1000, Loss: 426.06607284201107\n",
      "epoch: 600/1000, Loss: 415.8035696238082\n",
      "epoch: 700/1000, Loss: 406.43355943914713\n",
      "epoch: 800/1000, Loss: 397.85307843275626\n",
      "epoch: 900/1000, Loss: 389.55361621331997\n",
      "epoch: 1000/1000, Loss: 381.62643587810066\n",
      "epoch: 100/1000, Loss: 689.9251371044023\n",
      "epoch: 200/1000, Loss: 505.04835308248244\n",
      "epoch: 300/1000, Loss: 457.2869707943236\n",
      "epoch: 400/1000, Loss: 438.521677560703\n",
      "epoch: 500/1000, Loss: 426.7285642214507\n",
      "epoch: 600/1000, Loss: 416.8194647458746\n",
      "epoch: 700/1000, Loss: 407.7096726317882\n",
      "epoch: 800/1000, Loss: 399.3123860734442\n",
      "epoch: 900/1000, Loss: 391.0461384256817\n",
      "epoch: 1000/1000, Loss: 383.1730789820166\n",
      "epoch: 100/1000, Loss: 695.7653799553822\n",
      "epoch: 200/1000, Loss: 505.54645364980115\n",
      "epoch: 300/1000, Loss: 456.5181030364862\n",
      "epoch: 400/1000, Loss: 437.3181531052127\n",
      "epoch: 500/1000, Loss: 425.1088611182838\n",
      "epoch: 600/1000, Loss: 415.0498161982242\n",
      "epoch: 700/1000, Loss: 405.70679349540177\n",
      "epoch: 800/1000, Loss: 396.9829596426064\n",
      "epoch: 900/1000, Loss: 388.51257828675705\n",
      "epoch: 1000/1000, Loss: 380.2946794013844\n",
      "epoch: 100/1000, Loss: 754.8993998132064\n",
      "epoch: 200/1000, Loss: 567.4839139683836\n",
      "epoch: 300/1000, Loss: 515.0369698704969\n",
      "epoch: 400/1000, Loss: 492.28834685001397\n",
      "epoch: 500/1000, Loss: 476.3908386358629\n",
      "epoch: 600/1000, Loss: 462.7756863906224\n",
      "epoch: 700/1000, Loss: 450.5612953920604\n",
      "epoch: 800/1000, Loss: 439.61618176456307\n",
      "epoch: 900/1000, Loss: 429.82284181807586\n",
      "epoch: 1000/1000, Loss: 420.35505574979044\n",
      "epoch: 100/1000, Loss: 745.152656586783\n",
      "epoch: 200/1000, Loss: 562.2600365944916\n",
      "epoch: 300/1000, Loss: 511.03966583742493\n",
      "epoch: 400/1000, Loss: 488.09955717346514\n",
      "epoch: 500/1000, Loss: 472.4511548963089\n",
      "epoch: 600/1000, Loss: 459.18595812729075\n",
      "epoch: 700/1000, Loss: 447.21234746206653\n",
      "epoch: 800/1000, Loss: 436.3015963849565\n",
      "epoch: 900/1000, Loss: 425.970940211953\n",
      "epoch: 1000/1000, Loss: 416.7179861388255\n",
      "epoch: 100/1000, Loss: 748.7532008664515\n",
      "epoch: 200/1000, Loss: 564.1093345511176\n",
      "epoch: 300/1000, Loss: 512.5405442811048\n",
      "epoch: 400/1000, Loss: 489.8996382790639\n",
      "epoch: 500/1000, Loss: 474.46917665881557\n",
      "epoch: 600/1000, Loss: 461.44357636676716\n",
      "epoch: 700/1000, Loss: 449.51508391473993\n",
      "epoch: 800/1000, Loss: 438.1683268587429\n",
      "epoch: 900/1000, Loss: 427.8840027787486\n",
      "epoch: 1000/1000, Loss: 418.5917895954123\n",
      "epoch: 100/1000, Loss: 746.2591662342534\n",
      "epoch: 200/1000, Loss: 563.7566428859006\n",
      "epoch: 300/1000, Loss: 512.5884095663259\n",
      "epoch: 400/1000, Loss: 490.2339267363719\n",
      "epoch: 500/1000, Loss: 474.63706418534844\n",
      "epoch: 600/1000, Loss: 460.9800420587867\n",
      "epoch: 700/1000, Loss: 448.96506366652\n",
      "epoch: 800/1000, Loss: 438.16722128693283\n",
      "epoch: 900/1000, Loss: 428.39361615434944\n",
      "epoch: 1000/1000, Loss: 419.108234503084\n",
      "epoch: 100/1000, Loss: 738.3493609203919\n",
      "epoch: 200/1000, Loss: 557.7881923107652\n",
      "epoch: 300/1000, Loss: 507.1549360922761\n",
      "epoch: 400/1000, Loss: 484.87037197011625\n",
      "epoch: 500/1000, Loss: 469.45701314318995\n",
      "epoch: 600/1000, Loss: 456.46439019789466\n",
      "epoch: 700/1000, Loss: 444.58207638811916\n",
      "epoch: 800/1000, Loss: 433.26166272006355\n",
      "epoch: 900/1000, Loss: 423.5072946979646\n",
      "epoch: 1000/1000, Loss: 414.1482429965487\n",
      "epoch: 100/1000, Loss: 733.4822925999889\n",
      "epoch: 200/1000, Loss: 556.3441878571748\n",
      "epoch: 300/1000, Loss: 506.65302140296717\n",
      "epoch: 400/1000, Loss: 484.36700690505376\n",
      "epoch: 500/1000, Loss: 469.07626749794247\n",
      "epoch: 600/1000, Loss: 456.06468089473225\n",
      "epoch: 700/1000, Loss: 444.2836343969815\n",
      "epoch: 800/1000, Loss: 432.97785286145086\n",
      "epoch: 900/1000, Loss: 423.12560189905355\n",
      "epoch: 1000/1000, Loss: 413.8180871406895\n",
      "epoch: 100/1000, Loss: 749.9731588055945\n",
      "epoch: 200/1000, Loss: 566.9100210401547\n",
      "epoch: 300/1000, Loss: 515.7058832893031\n",
      "epoch: 400/1000, Loss: 493.4223855901773\n",
      "epoch: 500/1000, Loss: 478.0175797937794\n",
      "epoch: 600/1000, Loss: 465.13433129302325\n",
      "epoch: 700/1000, Loss: 453.3017592754469\n",
      "epoch: 800/1000, Loss: 442.731830721693\n",
      "epoch: 900/1000, Loss: 432.75770138002844\n",
      "epoch: 1000/1000, Loss: 423.59197078932806\n",
      "epoch: 100/1000, Loss: 744.6388439091478\n",
      "epoch: 200/1000, Loss: 563.4013848530371\n",
      "epoch: 300/1000, Loss: 512.2120086554232\n",
      "epoch: 400/1000, Loss: 489.4775359700596\n",
      "epoch: 500/1000, Loss: 473.9821121954359\n",
      "epoch: 600/1000, Loss: 461.01992544984296\n",
      "epoch: 700/1000, Loss: 449.0489552648578\n",
      "epoch: 800/1000, Loss: 438.2650073243602\n",
      "epoch: 900/1000, Loss: 428.34743311642353\n",
      "epoch: 1000/1000, Loss: 419.1749342325659\n",
      "epoch: 100/1000, Loss: 743.0678607530091\n",
      "epoch: 200/1000, Loss: 563.706426524282\n",
      "epoch: 300/1000, Loss: 513.3515939386392\n",
      "epoch: 400/1000, Loss: 491.28829224155993\n",
      "epoch: 500/1000, Loss: 476.00366121733\n",
      "epoch: 600/1000, Loss: 462.87568200010134\n",
      "epoch: 700/1000, Loss: 451.2387075270155\n",
      "epoch: 800/1000, Loss: 440.64428997799865\n",
      "epoch: 900/1000, Loss: 430.7894409446782\n",
      "epoch: 1000/1000, Loss: 421.6686827135717\n",
      "epoch: 100/1000, Loss: 746.9727162085846\n",
      "epoch: 200/1000, Loss: 562.9609050198897\n",
      "epoch: 300/1000, Loss: 511.6628732222451\n",
      "epoch: 400/1000, Loss: 489.07067153410344\n",
      "epoch: 500/1000, Loss: 473.6148048138511\n",
      "epoch: 600/1000, Loss: 460.17680981335985\n",
      "epoch: 700/1000, Loss: 448.24783537209464\n",
      "epoch: 800/1000, Loss: 437.5243391050346\n",
      "epoch: 900/1000, Loss: 427.8493761456461\n",
      "epoch: 1000/1000, Loss: 418.5085096789867\n"
     ]
    }
   ],
   "source": [
    "penalty_values = [.001, .01, .1, 1 , 2, 3, 4]  # penalty values to test\n",
    "\n",
    "def grid_search_penalty(model_class, X, y, penalty_values):\n",
    "    results = []  # storing results for each penalty\n",
    "    best_mse = float('inf')  # setting best_mse as a high value\n",
    "    non_zero_weights = 0\n",
    "    \n",
    "    # iterating through each penalty value\n",
    "    for penalty in penalty_values:\n",
    "        # cross-validating the model and getting the average MSE and non-zero weights\n",
    "        avg_mse, non_zero_weights = cross_validate_model(model_class, X, y, penalty)\n",
    "        results.append((penalty, avg_mse, non_zero_weights))\n",
    "\n",
    "        # Check if the current MSE is the lowest\n",
    "        if avg_mse < best_mse:\n",
    "            best_mse = avg_mse # updating best mse\n",
    "            best_penalty = penalty # updating penalty value for lowest mse\n",
    "            non_zero_weights = non_zero_weights # updating non-zero weighs count for lowest mse\n",
    "\n",
    "    return best_penalty, best_mse, non_zero_weights\n",
    "\n",
    "# Perform grid search for each model\n",
    "elastic_net_results = grid_search_penalty(EN.ElasticNet, X_scaled, y, penalty_values)\n",
    "sqrt_lasso_results = grid_search_penalty(SQRT.SqrtLasso, X_scaled, y, penalty_values)\n",
    "scad_results = grid_search_penalty(SCAD.SCADLinearRegression, X_scaled, y, penalty_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet - Best Penalty: 0.001, MSE: 113.71899704909735, Non-zero weights: 37.0\n",
      "SqrtLasso - Best Penalty: 0.001, MSE: 135.01281627102452, Non-zero weights: 37.0\n",
      "SCAD - Best Penalty: 0.1, MSE: 178.72833728279727, Non-zero weights: 37.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"ElasticNet - Best Penalty: {elastic_net_results[0]}, MSE: {elastic_net_results[1]}, Non-zero weights: {elastic_net_results[2]}\")\n",
    "print(f\"SqrtLasso - Best Penalty: {sqrt_lasso_results[0]}, MSE: {sqrt_lasso_results[1]}, Non-zero weights: {elastic_net_results[2]}\")\n",
    "print(f\"SCAD - Best Penalty: {scad_results[0]}, MSE: {scad_results[1]}, Non-zero weights: {elastic_net_results[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "Based on a very simple grid search, the best penalty value for ElasticNet and Square Root Lasso was found to be .001. For SCAD it was found to be 0.1. For all three \"best\" models, there were 37 non-zero weights. This means that there were 37 features that are contributing to the model. \n",
    "\n",
    "The model with the lowest MSE was ElasticNet with an MSE of 113.719 and 37 non-zero weights. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

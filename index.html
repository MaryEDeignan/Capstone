---
layout: home
author_profile: false
---

<h1 id="capstone">Capstone Projects</h1>
<h3 id="hw-1">HW 1</h3>
<p>Based on the examples provided, make your own class for implementing locally weighted regression to work with multiple features, and also train and test data. Show an application to a real data set with your implementation, and present the 10-fold cross-validated mean square error.</p>
<p>Find my HW1 code <a href="https://github.com/MaryEDeignan/Capstone/blob/main/HW1/HW1.ipynb">here</a> and the data I used  <a href="https://github.com/MaryEDeignan/Capstone/blob/main/HW1/cars.csv">here!</a> <br></p>

<h3 id="homework-2">Homework 2</h3>
<h4 id="part-1">Part 1</h4>
<p>Create your class that implements the Gradient Boosting concept, based on the locally weighted regression method (Lowess class), and that allows a user-prescribed number of boosting steps. The class you develop should have all the mainstream useful options, including “fit,” “is_fitted”, and “predict,” methods. Show applications with real data for regression, 10-fold cross-validations and compare the effect of different scalers, such as the “StandardScaler”, “MinMaxScaler”, and the “QuantileScaler”. In the case of the “Concrete” data set, determine a choice of hyperparameters that yield lower MSEs for your method when compared to the eXtream Gradient Boosting library.</p>
<h4 id="part-2">Part 2</h4>
<p>Implement your own version of Locally Weighted Logistic Regression and compare its performance on the Iris data set with the version presented in this article: <a href="https://calvintchi.github.io/classical_machine_learning/2020/08/16/lwlr.html">https://calvintchi.github.io/classical_machine_learning/2020/08/16/lwlr.html</a>.</p>
<p><strong>Present your results with detailed explanations and visualizations of the results in the format of a data science paper on Github.</strong> The link to my website with this Homework is <a href="https://maryedeignan.github.io/Homework2/">here!</a></p>

